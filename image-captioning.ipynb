{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install timm torch torchvision torch-geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:01:52.136030Z","iopub.execute_input":"2025-05-30T03:01:52.136199Z","iopub.status.idle":"2025-05-30T03:03:18.528213Z","shell.execute_reply.started":"2025-05-30T03:01:52.136184Z","shell.execute_reply":"2025-05-30T03:03:18.527150Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.18)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch-geometric\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-geometric-2.6.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import Tuple, List\nimport math\n\nclass SwinTransformerPatchEmbedding(nn.Module):\n    \"\"\"Simplified Swin Transformer patch embedding for feature extraction\"\"\"\n    def __init__(self, patch_size: int = 4, embed_dim: int = 96):\n        super().__init__()\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        # x: [B, C, H, W]\n        x = self.proj(x)  # [B, embed_dim, H//patch_size, W//patch_size]\n        B, C, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)  # [B, H*W, C]\n        return x, (H, W)\n\nclass GridGraphFeature(nn.Module):\n    \"\"\"\n    Grid-graph feature extraction based on the paper description\n    \"\"\"\n    def __init__(self,\n                 image_size: int = 224,\n                 patch_size: int = 4,\n                 embed_dim: int = 96,\n                 gcn_hidden_dim: int = 128):\n        super().__init__()\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.gcn_hidden_dim = gcn_hidden_dim\n\n        # Calculate number of patches\n        self.num_patches_per_side = image_size // patch_size  # M = k = 56 for 224/4\n        self.num_patches = self.num_patches_per_side ** 2     # M*k patches total\n\n        # Swin Transformer for patch feature extraction\n        self.swin_embedding = SwinTransformerPatchEmbedding(patch_size, embed_dim)\n\n        # Graph Convolutional Layer\n        self.gcn = GraphConvolutionalLayer(embed_dim, gcn_hidden_dim)\n\n    def extract_patch_features(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Extract patch features using Swin Transformer\n        Args:\n            image: [B, 3, 224, 224] input image\n        Returns:\n            X^V: [B, M*k, embed_dim] patch feature matrix\n        \"\"\"\n        # Split image into M x k patches and extract features\n        patch_features, (H, W) = self.swin_embedding(image)  # [B, M*k, embed_dim]\n\n        return patch_features\n\n    def compute_similarity_matrix(self, patch_features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute similarity matrix between patches\n        Args:\n            patch_features: [B, M*k, embed_dim] patch features\n        Returns:\n            A^V: [B, M*k, M*k] adjacency matrix\n        \"\"\"\n        B, N, D = patch_features.shape\n\n        # Normalize patch features for cosine similarity\n        patch_features_norm = F.normalize(patch_features, p=2, dim=-1)\n\n        # Compute cosine similarity matrix\n        similarity_matrix = torch.bmm(patch_features_norm, patch_features_norm.transpose(-2, -1))\n\n        # Create adjacency matrix based on similarity\n        # A^V_ij = 1 if |i-j| = 1 or |i-j| = k (adjacent patches)\n        #        = sim(i,j) otherwise\n        adjacency_matrix = torch.zeros_like(similarity_matrix)\n\n        for i in range(N):\n            for j in range(N):\n                # Convert 1D indices to 2D grid coordinates\n                i_row, i_col = i // self.num_patches_per_side, i % self.num_patches_per_side\n                j_row, j_col = j // self.num_patches_per_side, j % self.num_patches_per_side\n\n                # Check if patches are adjacent (horizontally or vertically)\n                if abs(i_row - j_row) + abs(i_col - j_col) == 1:\n                    adjacency_matrix[:, i, j] = 1.0\n                elif i == j:\n                    adjacency_matrix[:, i, j] = 1.0\n                else:\n                    adjacency_matrix[:, i, j] = similarity_matrix[:, i, j]\n        return adjacency_matrix\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of grid-graph feature extraction\n        Args:\n            image: [B, 3, 224, 224] input image\n        Returns:\n            V^(0): [B, M*k, hidden_dim] initial context grid representation\n        \"\"\"\n        # Step 1: Extract patch features using Swin Transformer\n        patch_features = self.extract_patch_features(image)  # X^V\n\n        # Step 2: Compute similarity-based adjacency matrix\n        adjacency_matrix = self.compute_similarity_matrix(patch_features)  # A^V\n\n        # Step 3: Apply Graph Convolutional Layer\n        grid_representation = self.gcn(patch_features, adjacency_matrix)  # V^(0)\n\n        return grid_representation\n\nclass GraphConvolutionalLayer(nn.Module):\n    \"\"\"\n    Graph Convolutional Layer implementation\n    V^(0) = σ(Ã^T VW^T)\n    \"\"\"\n    def __init__(self, input_dim: int, output_dim: int):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        # Weight matrix W\n        self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, node_features: torch.Tensor, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            node_features: [B, N, input_dim] node feature matrix V\n            adjacency_matrix: [B, N, N] adjacency matrix A^V\n        Returns:\n            output: [B, N, output_dim] output features V^(0)\n        \"\"\"\n        # Compute degree matrix D\n        degree_matrix = torch.sum(adjacency_matrix, dim=-1, keepdim=True)  # [B, N, 1]\n        degree_matrix = torch.clamp(degree_matrix, min=1.0)  # Avoid division by zero\n\n        # Normalize adjacency matrix: Ã = D^(-1/2) A^V D^(-1/2)\n        degree_inv_sqrt = torch.pow(degree_matrix, -0.5)\n        normalized_adj = degree_inv_sqrt * adjacency_matrix * degree_inv_sqrt.transpose(-2, -1)\n\n        # Apply weight transformation: VW^T\n        transformed_features = torch.matmul(node_features, self.weight)  # [B, N, output_dim]\n\n        # Graph convolution: Ã^T VW^T\n        output = torch.bmm(normalized_adj.transpose(-2, -1), transformed_features)\n\n        # Apply activation function (ReLU)\n        output = F.relu(output)\n\n        return output\n\n# Example usage and testing\ndef test_grid_graph_feature():\n    \"\"\"Test the Grid-Graph Feature implementation\"\"\"\n\n    # Create model\n    model = GridGraphFeature(\n        image_size=224,\n        patch_size=4,\n        embed_dim=96,\n        gcn_hidden_dim=128\n    )\n\n    # Create dummy input\n    batch_size = 2\n    dummy_image = torch.randn(batch_size, 3, 224, 224)\n\n    print(f\"Input image shape: {dummy_image.shape}\")\n    print(f\"Number of patches: {model.num_patches}\")\n    print(f\"Patches per side: {model.num_patches_per_side}\")\n\n    # Forward pass\n    with torch.no_grad():\n        grid_representation = model(dummy_image)\n\n    print(f\"Output grid representation shape: {grid_representation.shape}\")\n    print(f\"Expected shape: [{batch_size}, {model.num_patches}, {model.gcn_hidden_dim}]\")\n\n    # Test individual components\n    print(\"\\n--- Testing individual components ---\")\n\n    # Test patch feature extraction\n    patch_features = model.extract_patch_features(dummy_image)\n    print(f\"Patch features shape: {patch_features.shape}\")\n\n    # Test similarity matrix computation\n    similarity_matrix = model.compute_similarity_matrix(patch_features)\n    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n\n    # Verify adjacency matrix properties\n    print(f\"Adjacency matrix diagonal sum: {torch.diagonal(similarity_matrix, dim1=-2, dim2=-1).sum()}\")\n    print(f\"Adjacency matrix range: [{similarity_matrix.min():.3f}, {similarity_matrix.max():.3f}]\")\n\nif __name__ == \"__main__\":\n    test_grid_graph_feature()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T01:51:49.840028Z","iopub.execute_input":"2025-05-30T01:51:49.840236Z","execution_failed":"2025-05-30T02:01:33.473Z"}},"outputs":[{"name":"stdout","text":"Input image shape: torch.Size([2, 3, 224, 224])\nNumber of patches: 3136\nPatches per side: 56\nOutput grid representation shape: torch.Size([2, 3136, 128])\nExpected shape: [2, 3136, 128]\n\n--- Testing individual components ---\nPatch features shape: torch.Size([2, 3136, 96])\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom typing import Tuple, List, Dict\nimport torchvision.ops as ops\n\nclass FasterRCNN(nn.Module):\n    \"\"\"Simplified Faster R-CNN for object region detection\"\"\"\n    def __init__(self, backbone_dim: int = 512, num_classes: int = 80):\n        super().__init__()\n        self.backbone_dim = backbone_dim\n        self.num_classes = num_classes\n\n        # Simplified backbone (normally ResNet)\n        self.backbone = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, backbone_dim, 3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n        # ROI pooling output dimension\n        self.roi_pool_size = 7\n        self.roi_pooling = ops.RoIPool(output_size=(self.roi_pool_size, self.roi_pool_size), spatial_scale=1/16)\n\n        # Feature extraction after ROI pooling\n        self.roi_head = nn.Sequential(\n            nn.Linear(backbone_dim * self.roi_pool_size * self.roi_pool_size, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, images: torch.Tensor, boxes: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Extract features for given region boxes\n        Args:\n            images: [B, 3, H, W] input images\n            boxes: [B, P, 4] region boxes in format [x1, y1, x2, y2]\n        Returns:\n            region_features: [B, P, 512] features for each region\n        \"\"\"\n        # Extract backbone features\n        backbone_features = self.backbone(images)  # [B, backbone_dim, H', W']\n\n        B, P, _ = boxes.shape\n        region_features = []\n\n        for b in range(B):\n            # Prepare boxes for ROI pooling (add batch index)\n            batch_boxes = torch.cat([\n                torch.full((P, 1), b, dtype=boxes.dtype, device=boxes.device),\n                boxes[b]\n            ], dim=1)  # [P, 5] format: [batch_idx, x1, y1, x2, y2]\n\n            # ROI pooling\n            pooled_features = self.roi_pooling(backbone_features[b:b+1], batch_boxes)  # [P, backbone_dim, 7, 7]\n\n            # Flatten and process through ROI head\n            pooled_flat = pooled_features.view(P, -1)  # [P, backbone_dim * 7 * 7]\n            roi_features = self.roi_head(pooled_flat)  # [P, 512]\n\n            region_features.append(roi_features)\n\n        return torch.stack(region_features, dim=0)  # [B, P, 512]\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention mechanism for region features\"\"\"\n    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Linear projections for Q, K, V\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n\n        # Output projection\n        self.w_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(self.d_k)\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            query, key, value: [B, P, d_model] region features\n        Returns:\n            output: [B, P, d_model] attended features\n        \"\"\"\n        B, P, d_model = query.shape\n\n        # Linear projections\n        Q = self.w_q(query)  # [B, P, d_model]\n        K = self.w_k(key)    # [B, P, d_model]\n        V = self.w_v(value)  # [B, P, d_model]\n\n        # Reshape for multi-head attention\n        Q = Q.view(B, P, self.num_heads, self.d_k).transpose(1, 2)  # [B, num_heads, P, d_k]\n        K = K.view(B, P, self.num_heads, self.d_k).transpose(1, 2)  # [B, num_heads, P, d_k]\n        V = V.view(B, P, self.num_heads, self.d_k).transpose(1, 2)  # [B, num_heads, P, d_k]\n\n        # Scaled dot-product attention\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [B, num_heads, P, P]\n        attention_weights = F.softmax(attention_scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # Apply attention to values\n        attended_values = torch.matmul(attention_weights, V)  # [B, num_heads, P, d_k]\n\n        # Concatenate heads\n        attended_values = attended_values.transpose(1, 2).contiguous().view(B, P, d_model)\n\n        # Final linear projection\n        output = self.w_o(attended_values)\n\n        return output\n\nclass SelfAttention(nn.Module):\n    \"\"\"Self-Attention mechanism as described in the paper\"\"\"\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n        self.scale = math.sqrt(d_model)\n\n    def forward(self, R: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute QK^T Self-Attention\n        Args:\n            R: [B, P, d] region features where Q = K = V = R\n        Returns:\n            attention_output: [B, P, d] attended features\n        \"\"\"\n        # Q = K = V = R\n        Q = K = V = R  # [B, P, d]\n\n        # Compute attention scores: QK^T / sqrt(d)\n        attention_scores = torch.bmm(Q, K.transpose(-2, -1)) / self.scale  # [B, P, P]\n\n        # Apply softmax\n        attention_weights = F.softmax(attention_scores, dim=-1)  # [B, P, P]\n\n        # Apply attention to values\n        attention_output = torch.bmm(attention_weights, V)  # [B, P, d]\n\n        return attention_output\n\nclass RegionGraphFeature(nn.Module):\n    \"\"\"\n    Region Graph Feature extraction using Faster R-CNN and Multi-Head Attention\n    \"\"\"\n    def __init__(self,\n                 num_regions: int = 100,\n                 region_feature_dim: int = 512,\n                 num_attention_heads: int = 8,\n                 output_dim: int = 512):\n        super().__init__()\n\n        self.num_regions = num_regions\n        self.region_feature_dim = region_feature_dim\n        self.num_attention_heads = num_attention_heads\n        self.output_dim = output_dim\n\n        # Faster R-CNN for region detection and feature extraction\n        self.faster_rcnn = FasterRCNN(backbone_dim=512, num_classes=80)\n\n        # Self-Attention mechanism\n        self.self_attention = SelfAttention(region_feature_dim)\n\n        # Multi-Head Attention mechanism\n        self.multi_head_attention = MultiHeadAttention(\n            d_model=region_feature_dim,\n            num_heads=num_attention_heads\n        )\n\n        # Final projection layer\n        self.output_projection = nn.Linear(region_feature_dim, output_dim)\n\n    def generate_dummy_boxes(self, batch_size: int, num_boxes: int, image_size: Tuple[int, int]) -> torch.Tensor:\n        \"\"\"\n        Generate dummy bounding boxes for testing\n        In practice, these would come from Faster R-CNN's RPN\n        \"\"\"\n        H, W = image_size\n        boxes = []\n\n        for _ in range(batch_size):\n            batch_boxes = []\n            for _ in range(num_boxes):\n                # Random box coordinates\n                x1 = torch.randint(0, W//2, (1,)).float()\n                y1 = torch.randint(0, H//2, (1,)).float()\n                x2 = x1 + torch.randint(W//4, W//2, (1,)).float()\n                y2 = y1 + torch.randint(H//4, H//2, (1,)).float()\n\n                # Ensure boxes are within image bounds\n                x2 = torch.clamp(x2, max=W-1)\n                y2 = torch.clamp(y2, max=H-1)\n\n                batch_boxes.append(torch.stack([x1, y1, x2, y2]))\n\n            boxes.append(torch.stack(batch_boxes).squeeze())\n\n        return torch.stack(boxes)\n\n    def forward(self, images: torch.Tensor, region_boxes: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass for region graph feature extraction\n        Args:\n            images: [B, 3, H, W] input images\n            region_boxes: [B, P, 4] region boxes (optional, will generate dummy if None)\n        Returns:\n            R^(0): [B, P, output_dim] initial region representation\n        \"\"\"\n        B, C, H, W = images.shape\n\n        # Generate dummy boxes if not provided (in practice, use Faster R-CNN's RPN)\n        if region_boxes is None:\n            region_boxes = self.generate_dummy_boxes(B, self.num_regions, (H, W))\n            region_boxes = region_boxes.to(images.device)\n\n        # Step 1: Extract region features using Faster R-CNN\n        region_features = self.faster_rcnn(images, region_boxes)  # R: [B, P, d]\n\n        # Step 2: Apply Self-Attention mechanism\n        # QK^T Attention where Q = K = V = R\n        sa_output = self.self_attention(region_features)  # [B, P, d]\n\n        # Step 3: Apply Multi-Head Attention for more comprehensive interaction\n        mha_output = self.multi_head_attention(sa_output, sa_output, sa_output)  # [B, P, d]\n\n        # Step 4: Combine with residual connection\n        combined_features = region_features + mha_output  # [B, P, d]\n\n        # Step 5: Final projection to get R^(0)\n        initial_region_representation = self.output_projection(combined_features)  # [B, P, output_dim]\n\n        return initial_region_representation\n\n    def get_region_vectors(self, region_representation: torch.Tensor) -> List[torch.Tensor]:\n        \"\"\"\n        Extract individual region vectors {r_i}\n        Args:\n            region_representation: [B, P, d] region features\n        Returns:\n            List of region vectors for each batch\n        \"\"\"\n        B, P, d = region_representation.shape\n        region_vectors = []\n\n        for b in range(B):\n            batch_vectors = [region_representation[b, p, :] for p in range(P)]\n            region_vectors.append(batch_vectors)\n\n        return region_vectors\n\n# Example usage and testing\ndef test_region_graph_feature():\n    \"\"\"Test the Region Graph Feature implementation\"\"\"\n\n    # Create model\n    model = RegionGraphFeature(\n        num_regions=50,\n        region_feature_dim=512,\n        num_attention_heads=8,\n        output_dim=512\n    )\n\n    # Create dummy input\n    batch_size = 2\n    dummy_images = torch.randn(batch_size, 3, 224, 224)\n\n    print(f\"Input images shape: {dummy_images.shape}\")\n    print(f\"Number of regions: {model.num_regions}\")\n    print(f\"Region feature dimension: {model.region_feature_dim}\")\n\n    # Forward pass\n    with torch.no_grad():\n        region_representation = model(dummy_images)\n\n    print(f\"Output region representation shape: {region_representation.shape}\")\n    print(f\"Expected shape: [{batch_size}, {model.num_regions}, {model.output_dim}]\")\n\n    # Test with custom boxes\n    print(\"\\n--- Testing with custom region boxes ---\")\n    custom_boxes = torch.tensor([\n        [[10, 10, 50, 50], [60, 60, 100, 100], [120, 30, 180, 90]],\n        [[20, 20, 80, 80], [90, 10, 150, 70], [30, 100, 90, 160]]\n    ], dtype=torch.float32)\n\n    with torch.no_grad():\n        custom_region_representation = model(dummy_images, custom_boxes)\n\n    print(f\"Custom region representation shape: {custom_region_representation.shape}\")\n\n    # Test individual components\n    print(\"\\n--- Testing individual components ---\")\n\n    # Test Faster R-CNN feature extraction\n    dummy_boxes = model.generate_dummy_boxes(batch_size, 10, (224, 224))\n    region_features = model.faster_rcnn(dummy_images, dummy_boxes)\n    print(f\"Faster R-CNN features shape: {region_features.shape}\")\n\n    # Test Self-Attention\n    sa_output = model.self_attention(region_features)\n    print(f\"Self-Attention output shape: {sa_output.shape}\")\n\n    # Test Multi-Head Attention\n    mha_output = model.multi_head_attention(region_features, region_features, region_features)\n    print(f\"Multi-Head Attention output shape: {mha_output.shape}\")\n\n    # Extract individual region vectors\n    region_vectors = model.get_region_vectors(region_representation)\n    print(f\"Number of region vector batches: {len(region_vectors)}\")\n    print(f\"Number of vectors per batch: {len(region_vectors[0])}\")\n    print(f\"Each region vector shape: {region_vectors[0][0].shape}\")\n\nif __name__ == \"__main__\":\n    test_region_graph_feature()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:03:51.464379Z","iopub.execute_input":"2025-05-30T03:03:51.465311Z","execution_failed":"2025-05-30T03:03:55.846Z"}},"outputs":[{"name":"stdout","text":"Input images shape: torch.Size([2, 3, 224, 224])\nNumber of regions: 50\nRegion feature dimension: 512\nOutput region representation shape: torch.Size([2, 50, 512])\nExpected shape: [2, 50, 512]\n\n--- Testing with custom region boxes ---\nCustom region representation shape: torch.Size([2, 3, 512])\n\n--- Testing individual components ---\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nimport math\nfrom typing import List, Dict, Tuple, Optional\nimport spacy\nimport networkx as nx\n\nclass BERTEmbedding(nn.Module):\n    \"\"\"BERT-based text embedding for semantic features\"\"\"\n    def __init__(self, model_name: str = 'bert-base-uncased', embedding_dim: int = 768):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n        self.bert_model = BertModel.from_pretrained(model_name)\n\n        # Freeze BERT parameters for efficiency (optional)\n        for param in self.bert_model.parameters():\n            param.requires_grad = False\n\n    def forward(self, text_sequences: List[str]) -> torch.Tensor:\n        \"\"\"\n        Convert text sequences to BERT embeddings\n        Args:\n            text_sequences: List of text strings\n        Returns:\n            embeddings: [N, embedding_dim] where N is number of sequences\n        \"\"\"\n        # Tokenize texts\n        encoded = self.tokenizer(\n            text_sequences,\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors='pt'\n        )\n\n        # Get BERT embeddings\n        with torch.no_grad():\n            outputs = self.bert_model(**encoded)\n            # Use [CLS] token embedding as sentence representation\n            embeddings = outputs.last_hidden_state[:, 0, :]  # [N, embedding_dim]\n\n        return embeddings\n\nclass DependencyParser:\n    \"\"\"Dependency parsing for constructing semantic graphs\"\"\"\n    def __init__(self):\n        # Load spaCy model for dependency parsing\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            print(\"Warning: spaCy English model not found. Using dummy parser.\")\n            self.nlp = None\n\n    def parse_text(self, text: str) -> List[Dict]:\n        \"\"\"\n        Parse text and extract dependency relationships\n        Args:\n            text: Input text string\n        Returns:\n            dependencies: List of dependency relations\n        \"\"\"\n        if self.nlp is None:\n            # Dummy dependencies for testing\n            words = text.split()\n            return [{\"head\": i, \"child\": i+1, \"relation\": \"dummy\"}\n                   for i in range(len(words)-1)]\n\n        doc = self.nlp(text)\n        dependencies = []\n\n        for token in doc:\n            if token.head != token:  # Skip root\n                dependencies.append({\n                    \"head\": token.head.i,\n                    \"child\": token.i,\n                    \"relation\": token.dep_,\n                    \"head_text\": token.head.text,\n                    \"child_text\": token.text\n                })\n\n        return dependencies\n\n    def extract_words(self, text: str) -> List[str]:\n        \"\"\"Extract individual words from text\"\"\"\n        if self.nlp is None:\n            return text.split()\n\n        doc = self.nlp(text)\n        return [token.text for token in doc if not token.is_punct]\n\nclass SemanticGraphConstructor:\n    \"\"\"Construct semantic graphs from dependency trees\"\"\"\n    def __init__(self):\n        self.dependency_parser = DependencyParser()\n\n    def construct_adjacency_matrix(self, text: str) -> Tuple[torch.Tensor, List[str]]:\n        \"\"\"\n        Construct adjacency matrix from dependency tree\n        Args:\n            text: Input text string\n        Returns:\n            adjacency_matrix: [N, N] adjacency matrix\n            words: List of words corresponding to matrix indices\n        \"\"\"\n        # Extract words and dependencies\n        words = self.dependency_parser.extract_words(text)\n        dependencies = self.dependency_parser.parse_text(text)\n\n        N = len(words)\n        if N == 0:\n            return torch.zeros(1, 1), [\"\"]\n\n        # Initialize adjacency matrix\n        adjacency_matrix = torch.zeros(N, N)\n\n        # Fill adjacency matrix based on dependencies\n        for dep in dependencies:\n            head_idx = dep[\"head\"]\n            child_idx = dep[\"child\"]\n\n            # Ensure indices are within bounds\n            if 0 <= head_idx < N and 0 <= child_idx < N:\n                # Undirected graph: set both directions\n                adjacency_matrix[head_idx, child_idx] = 1.0\n                adjacency_matrix[child_idx, head_idx] = 1.0\n\n        # Add self-connections (diagonal)\n        for i in range(N):\n            adjacency_matrix[i, i] = 1.0\n\n        return adjacency_matrix, words\n\n    def correlation_function(self, wi: str, wj: str) -> float:\n        \"\"\"\n        Compute correlation between two words D(wi, wj)\n        This is a simplified implementation - in practice, could use:\n        - Word embeddings similarity\n        - Co-occurrence statistics\n        - Semantic similarity measures\n        \"\"\"\n        # Simple correlation based on string similarity and length\n        if wi == wj:\n            return 1.0\n\n        # Jaccard similarity of character sets\n        set_i = set(wi.lower())\n        set_j = set(wj.lower())\n\n        if len(set_i.union(set_j)) == 0:\n            return 0.0\n\n        correlation = len(set_i.intersection(set_j)) / len(set_i.union(set_j))\n        return correlation\n\nclass SemanticGraphConvolutionalLayer(nn.Module):\n    \"\"\"\n    Semantic Graph Convolutional Layer\n    S^(0) = σ(Ã^S σ(Ã^S SW₁^S) W₂^S)\n    \"\"\"\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n\n        # Weight matrices\n        self.W1 = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n        self.W2 = nn.Parameter(torch.FloatTensor(hidden_dim, output_dim))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv1 = 1. / math.sqrt(self.W1.size(1))\n        self.W1.data.uniform_(-stdv1, stdv1)\n\n        stdv2 = 1. / math.sqrt(self.W2.size(1))\n        self.W2.data.uniform_(-stdv2, stdv2)\n\n    def normalize_adjacency_matrix(self, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Normalize adjacency matrix: Ã = D^(-1/2) A D^(-1/2)\n        \"\"\"\n        # Compute degree matrix\n        degree = torch.sum(adjacency_matrix, dim=-1, keepdim=True)  # [N, 1]\n        degree = torch.clamp(degree, min=1.0)  # Avoid division by zero\n\n        # D^(-1/2)\n        degree_inv_sqrt = torch.pow(degree, -0.5)\n\n        # Normalize: D^(-1/2) A D^(-1/2)\n        normalized_adj = degree_inv_sqrt * adjacency_matrix * degree_inv_sqrt.transpose(-2, -1)\n\n        return normalized_adj\n\n    def forward(self, node_features: torch.Tensor, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            node_features: [N, input_dim] node features S\n            adjacency_matrix: [N, N] adjacency matrix A^S\n        Returns:\n            output: [N, output_dim] output features S^(0)\n        \"\"\"\n        # Normalize adjacency matrix\n        normalized_adj = self.normalize_adjacency_matrix(adjacency_matrix)  # Ã^S\n\n        # First GCN layer: Ã^S S W₁^S\n        h1 = torch.matmul(node_features, self.W1)  # [N, hidden_dim]\n        h1 = torch.matmul(normalized_adj, h1)      # [N, hidden_dim]\n        h1 = F.relu(h1)  # σ(Ã^S S W₁^S)\n\n        # Second GCN layer: Ã^S σ(Ã^S S W₁^S) W₂^S\n        h2 = torch.matmul(h1, self.W2)             # [N, output_dim]\n        output = torch.matmul(normalized_adj, h2)   # [N, output_dim]\n        output = F.relu(output)  # σ(Ã^S σ(Ã^S S W₁^S) W₂^S)\n\n        return output\n\nclass SemanticGraphFeature(nn.Module):\n    \"\"\"\n    Complete Semantic Graph Feature extraction pipeline\n    \"\"\"\n    def __init__(self,\n                 bert_model_name: str = 'bert-base-uncased',\n                 bert_dim: int = 768,\n                 gcn_hidden_dim: int = 512,\n                 output_dim: int = 256):\n        super().__init__()\n\n        self.bert_dim = bert_dim\n        self.gcn_hidden_dim = gcn_hidden_dim\n        self.output_dim = output_dim\n\n        # BERT embedding for word-level features\n        self.bert_embedding = BERTEmbedding(bert_model_name, bert_dim)\n\n        # Semantic graph constructor\n        self.graph_constructor = SemanticGraphConstructor()\n\n        # Semantic Graph Convolutional Layer\n        self.semantic_gcn = SemanticGraphConvolutionalLayer(\n            input_dim=bert_dim,\n            hidden_dim=gcn_hidden_dim,\n            output_dim=output_dim\n        )\n\n    def process_single_text(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Process a single text to extract semantic graph features\n        Args:\n            text: Input text string\n        Returns:\n            semantic_features: [N, output_dim] semantic node features\n            adjacency_matrix: [N, N] semantic adjacency matrix\n        \"\"\"\n        # Step 1: Construct semantic graph\n        adjacency_matrix, words = self.graph_constructor.construct_adjacency_matrix(text)\n\n        if len(words) == 0 or words == [\"\"]:\n            # Handle empty text\n            return torch.zeros(1, self.output_dim), torch.zeros(1, 1)\n\n        # Step 2: Get BERT embeddings for words\n        word_embeddings = self.bert_embedding(words)  # [N, bert_dim]\n\n        # Step 3: Apply Semantic GCN\n        semantic_features = self.semantic_gcn(word_embeddings, adjacency_matrix)  # [N, output_dim]\n\n        return semantic_features, adjacency_matrix\n\n    def forward(self, text_list: List[str]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n        \"\"\"\n        Process multiple texts\n        Args:\n            text_list: List of text strings\n        Returns:\n            semantic_features_list: List of [Ni, output_dim] tensors\n            adjacency_matrices_list: List of [Ni, Ni] tensors\n        \"\"\"\n        semantic_features_list = []\n        adjacency_matrices_list = []\n\n        for text in text_list:\n            semantic_features, adjacency_matrix = self.process_single_text(text)\n            semantic_features_list.append(semantic_features)\n            adjacency_matrices_list.append(adjacency_matrix)\n\n        return semantic_features_list, adjacency_matrices_list\n\n    def get_semantic_vectors(self, semantic_features_list: List[torch.Tensor]) -> List[List[torch.Tensor]]:\n        \"\"\"\n        Extract individual semantic vectors {s_i}\n        Args:\n            semantic_features_list: List of [Ni, output_dim] tensors\n        Returns:\n            List of lists containing individual semantic vectors\n        \"\"\"\n        semantic_vectors = []\n\n        for semantic_features in semantic_features_list:\n            N, d = semantic_features.shape\n            text_vectors = [semantic_features[i, :] for i in range(N)]\n            semantic_vectors.append(text_vectors)\n\n        return semantic_vectors\n\n# Example usage and testing\ndef test_semantic_graph_feature():\n    \"\"\"Test the Semantic Graph Feature implementation\"\"\"\n\n    print(\"Testing Semantic Graph Feature Implementation\")\n    print(\"=\" * 50)\n\n    # Create model\n    model = SemanticGraphFeature(\n        bert_model_name='bert-base-uncased',\n        bert_dim=768,\n        gcn_hidden_dim=512,\n        output_dim=256\n    )\n\n    # Test texts (image captions)\n    test_texts = [\n        \"A cat sitting on a wooden table\",\n        \"Two dogs playing in the park\",\n        \"Beautiful sunset over the ocean\",\n        \"Person riding bicycle on street\"\n    ]\n\n    print(f\"Test texts: {len(test_texts)} captions\")\n    for i, text in enumerate(test_texts):\n        print(f\"  {i+1}. {text}\")\n\n    # Process texts\n    semantic_features_list, adjacency_matrices_list = model(test_texts)\n\n    print(f\"\\nResults:\")\n    print(f\"Number of processed texts: {len(semantic_features_list)}\")\n\n    for i, (features, adj_matrix) in enumerate(zip(semantic_features_list, adjacency_matrices_list)):\n        print(f\"\\nText {i+1}: '{test_texts[i]}'\")\n        print(f\"  Semantic features shape: {features.shape}\")\n        print(f\"  Adjacency matrix shape: {adj_matrix.shape}\")\n        print(f\"  Number of words/nodes: {features.shape[0]}\")\n        print(f\"  Feature dimension: {features.shape[1]}\")\n        print(f\"  Graph density: {(adj_matrix.sum() - adj_matrix.trace()) / (adj_matrix.numel() - adj_matrix.shape[0]):.3f}\")\n\n    # Test individual components\n    print(f\"\\n\" + \"=\"*50)\n    print(\"Testing Individual Components\")\n    print(\"=\"*50)\n\n    # Test dependency parsing\n    test_text = \"A cat sitting on a wooden table\"\n    dependencies = model.graph_constructor.dependency_parser.parse_text(test_text)\n    words = model.graph_constructor.dependency_parser.extract_words(test_text)\n\n    print(f\"\\nDependency parsing for: '{test_text}'\")\n    print(f\"Words: {words}\")\n    print(f\"Dependencies: {len(dependencies)}\")\n    for dep in dependencies[:3]:  # Show first 3\n        print(f\"  {dep}\")\n\n    # Test adjacency matrix construction\n    adj_matrix, extracted_words = model.graph_constructor.construct_adjacency_matrix(test_text)\n    print(f\"\\nAdjacency matrix shape: {adj_matrix.shape}\")\n    print(f\"Extracted words: {extracted_words}\")\n    print(f\"Adjacency matrix:\\n{adj_matrix}\")\n\n    # Test BERT embeddings\n    word_embeddings = model.bert_embedding(words[:3])  # Test first 3 words\n    print(f\"\\nBERT embeddings shape for 3 words: {word_embeddings.shape}\")\n    print(f\"Embedding dimension: {word_embeddings.shape[1]}\")\n\n    # Test semantic vectors extraction\n    semantic_vectors = model.get_semantic_vectors(semantic_features_list)\n    print(f\"\\nSemantic vectors:\")\n    print(f\"Number of texts: {len(semantic_vectors)}\")\n    if len(semantic_vectors) > 0:\n        print(f\"Vectors in first text: {len(semantic_vectors[0])}\")\n        if len(semantic_vectors[0]) > 0:\n            print(f\"Each vector shape: {semantic_vectors[0][0].shape}\")\n\nif __name__ == \"__main__\":\n    test_semantic_graph_feature()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:04:22.351659Z","iopub.execute_input":"2025-05-30T03:04:22.351956Z","iopub.status.idle":"2025-05-30T03:04:53.097970Z","shell.execute_reply.started":"2025-05-30T03:04:22.351935Z","shell.execute_reply":"2025-05-30T03:04:53.097248Z"}},"outputs":[{"name":"stderr","text":"2025-05-30 03:04:31.419242: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748574271.616816     113 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748574271.671914     113 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Testing Semantic Graph Feature Implementation\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6ab5ff85864754a991f8c60622d7af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f970346fabc4b8bb2fca3d5e467e1ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc9e1f28d93461b816883b605578c39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"009c7557b73b4f6ab7834de9e01ba8d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07517e92b4b146a6b54dc7b23a1a409a"}},"metadata":{}},{"name":"stdout","text":"Test texts: 4 captions\n  1. A cat sitting on a wooden table\n  2. Two dogs playing in the park\n  3. Beautiful sunset over the ocean\n  4. Person riding bicycle on street\n\nResults:\nNumber of processed texts: 4\n\nText 1: 'A cat sitting on a wooden table'\n  Semantic features shape: torch.Size([7, 256])\n  Adjacency matrix shape: torch.Size([7, 7])\n  Number of words/nodes: 7\n  Feature dimension: 256\n  Graph density: 0.286\n\nText 2: 'Two dogs playing in the park'\n  Semantic features shape: torch.Size([6, 256])\n  Adjacency matrix shape: torch.Size([6, 6])\n  Number of words/nodes: 6\n  Feature dimension: 256\n  Graph density: 0.333\n\nText 3: 'Beautiful sunset over the ocean'\n  Semantic features shape: torch.Size([5, 256])\n  Adjacency matrix shape: torch.Size([5, 5])\n  Number of words/nodes: 5\n  Feature dimension: 256\n  Graph density: 0.400\n\nText 4: 'Person riding bicycle on street'\n  Semantic features shape: torch.Size([5, 256])\n  Adjacency matrix shape: torch.Size([5, 5])\n  Number of words/nodes: 5\n  Feature dimension: 256\n  Graph density: 0.400\n\n==================================================\nTesting Individual Components\n==================================================\n\nDependency parsing for: 'A cat sitting on a wooden table'\nWords: ['A', 'cat', 'sitting', 'on', 'a', 'wooden', 'table']\nDependencies: 6\n  {'head': 1, 'child': 0, 'relation': 'det', 'head_text': 'cat', 'child_text': 'A'}\n  {'head': 1, 'child': 2, 'relation': 'acl', 'head_text': 'cat', 'child_text': 'sitting'}\n  {'head': 2, 'child': 3, 'relation': 'prep', 'head_text': 'sitting', 'child_text': 'on'}\n\nAdjacency matrix shape: torch.Size([7, 7])\nExtracted words: ['A', 'cat', 'sitting', 'on', 'a', 'wooden', 'table']\nAdjacency matrix:\ntensor([[1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0.],\n        [0., 1., 1., 1., 0., 0., 0.],\n        [0., 0., 1., 1., 0., 0., 1.],\n        [0., 0., 0., 0., 1., 0., 1.],\n        [0., 0., 0., 0., 0., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1.]])\n\nBERT embeddings shape for 3 words: torch.Size([3, 768])\nEmbedding dimension: 768\n\nSemantic vectors:\nNumber of texts: 4\nVectors in first text: 7\nEach vector shape: torch.Size([256])\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom typing import Tuple, List, Dict, Optional\n\nclass RegionAggregationFeature(nn.Module):\n    \"\"\"\n    Region Aggregation Feature implementation\n    Aggregates grid features into representative regional centers\n    \"\"\"\n    def __init__(self, \n                 grid_feature_dim: int = 128,\n                 num_regions: int = 10,\n                 num_clusters: int = 5,\n                 temperature: float = 1.0):\n        super().__init__()\n        \n        self.grid_feature_dim = grid_feature_dim\n        self.num_regions = num_regions\n        self.num_clusters = num_clusters\n        self.temperature = temperature\n        \n        # Learnable cluster centers r_i^(0)\n        self.cluster_centers = nn.Parameter(\n            torch.randn(num_clusters, grid_feature_dim)\n        )\n        \n        # Adjustable parameters b_r and b_i\n        self.b_r = nn.Parameter(torch.zeros(1))\n        self.b_i = nn.Parameter(torch.zeros(1))\n        \n        # Learnable parameter r̃_i for weighted summing\n        self.r_tilde = nn.Parameter(torch.randn(grid_feature_dim))\n        \n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        \"\"\"Initialize parameters\"\"\"\n        # Initialize cluster centers with Xavier uniform\n        nn.init.xavier_uniform_(self.cluster_centers)\n        \n        # Initialize biases to zero\n        nn.init.zeros_(self.b_r)\n        nn.init.zeros_(self.b_i)\n        \n        # Initialize r̃_i with small random values\n        nn.init.normal_(self.r_tilde, mean=0.0, std=0.1)\n    \n    def compute_correlation_weights(self, \n                                   grid_features: torch.Tensor, \n                                   cluster_centers: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute correlation weights r_ji^(0) using softmax\n        Args:\n            grid_features: [B, M, grid_feature_dim] grid features v_j^(0)\n            cluster_centers: [C, grid_feature_dim] cluster centers r_i^(0)\n        Returns:\n            correlation_weights: [B, M, C] correlation weights r_ji^(0)\n        \"\"\"\n        B, M, D = grid_features.shape\n        C = cluster_centers.shape[0]\n        \n        # Expand dimensions for broadcasting\n        # grid_features: [B, M, 1, D]\n        # cluster_centers: [1, 1, C, D]\n        grid_expanded = grid_features.unsqueeze(2)  # [B, M, 1, D]\n        centers_expanded = cluster_centers.unsqueeze(0).unsqueeze(0)  # [1, 1, C, D]\n        \n        # Compute dot product: v_j^(0) · r_i^(0)\n        dot_products = torch.sum(grid_expanded * centers_expanded, dim=-1)  # [B, M, C]\n        \n        # Add adjustable parameters\n        # exp(v_j^(0) · r_i^(0) + b_r) for numerator when computing r_ji^(0)\n        numerator_logits = dot_products + self.b_r  # [B, M, C]\n        \n        # Compute softmax over cluster dimension\n        correlation_weights = F.softmax(numerator_logits / self.temperature, dim=-1)  # [B, M, C]\n        \n        return correlation_weights\n    \n    def compute_regional_features(self, \n                                  grid_features: torch.Tensor, \n                                  correlation_weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute regional features r_i^(1) using weighted summing\n        Args:\n            grid_features: [B, M, D] grid features v_j^(0)\n            correlation_weights: [B, M, C] correlation weights r_ji^(0)\n        Returns:\n            regional_features: [B, C, D] regional features r_i^(1)\n        \"\"\"\n        B, M, D = grid_features.shape\n        C = correlation_weights.shape[-1]\n        \n        # Weighted summing: Σ(j=1 to M) r_ji^(0) * (v_j^(0) - r̃_i)\n        # Expand r̃_i to match dimensions\n        r_tilde_expanded = self.r_tilde.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # [1, 1, 1, D]\n        \n        # Compute (v_j^(0) - r̃_i)\n        centered_features = grid_features.unsqueeze(2) - r_tilde_expanded  # [B, M, 1, D] - [1, 1, 1, D] = [B, M, 1, D]\n        \n        # Expand correlation weights for multiplication\n        weights_expanded = correlation_weights.unsqueeze(-1)  # [B, M, C, 1]\n        \n        # Weighted sum: r_ji^(0) * (v_j^(0) - r̃_i)\n        weighted_features = weights_expanded * centered_features.unsqueeze(2)  # [B, M, C, D]\n        \n        # Sum over grid positions\n        summed_features = torch.sum(weighted_features, dim=1)  # [B, C, D]\n        \n        # Apply L2 normalization\n        regional_features = F.normalize(summed_features, p=2, dim=-1)  # [B, C, D]\n        \n        return regional_features\n    \n    def forward(self, grid_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass for region aggregation\n        Args:\n            grid_features: [B, M, D] grid features from Section 3.1.1 (V^(0))\n        Returns:\n            regional_features: [B, C, D] aggregated regional features R^(1)\n            correlation_weights: [B, M, C] correlation weights for analysis\n        \"\"\"\n        # Step 1: Compute correlation weights r_ji^(0)\n        correlation_weights = self.compute_correlation_weights(grid_features, self.cluster_centers)\n        \n        # Step 2: Compute regional features r_i^(1)\n        regional_features = self.compute_regional_features(grid_features, correlation_weights)\n        \n        return regional_features, correlation_weights\n    \n    def get_cluster_assignments(self, correlation_weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Get hard cluster assignments for each grid position\n        Args:\n            correlation_weights: [B, M, C] soft correlation weights\n        Returns:\n            assignments: [B, M] cluster assignments (0 to C-1)\n        \"\"\"\n        return torch.argmax(correlation_weights, dim=-1)\n    \n    def compute_clustering_loss(self, \n                                grid_features: torch.Tensor, \n                                correlation_weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute clustering loss to encourage meaningful clustering\n        Args:\n            grid_features: [B, M, D] grid features\n            correlation_weights: [B, M, C] correlation weights\n        Returns:\n            loss: scalar clustering loss\n        \"\"\"\n        B, M, D = grid_features.shape\n        C = self.num_clusters\n        \n        # Compute cluster centers from weighted average\n        weights_sum = torch.sum(correlation_weights, dim=1, keepdim=True)  # [B, 1, C]\n        weights_sum = torch.clamp(weights_sum, min=1e-8)  # Avoid division by zero\n        \n        # Weighted average of features for each cluster\n        weighted_features = torch.sum(\n            grid_features.unsqueeze(2) * correlation_weights.unsqueeze(-1),\n            dim=1\n        )  # [B, C, D]\n        computed_centers = weighted_features / weights_sum.unsqueeze(-1)  # [B, C, D]\n        \n        # Compute intra-cluster variance\n        grid_expanded = grid_features.unsqueeze(2)  # [B, M, 1, D]\n        centers_expanded = computed_centers.unsqueeze(1)  # [B, 1, C, D]\n        \n        # Distance from each point to each cluster center\n        distances = torch.sum((grid_expanded - centers_expanded) ** 2, dim=-1)  # [B, M, C]\n        \n        # Weighted intra-cluster variance\n        weighted_variance = torch.sum(correlation_weights * distances) / (B * M)\n        \n        return weighted_variance\n\nclass RegionAggregationModule(nn.Module):\n    \"\"\"\n    Complete Region Aggregation Module with multiple aggregation methods\n    \"\"\"\n    def __init__(self, \n                 grid_feature_dim: int = 128,\n                 num_regions: int = 10,\n                 num_clusters: int = 5,\n                 aggregation_method: str = 'attention'):\n        super().__init__()\n        \n        self.grid_feature_dim = grid_feature_dim\n        self.num_regions = num_regions\n        self.num_clusters = num_clusters\n        self.aggregation_method = aggregation_method\n        \n        # Main region aggregation component\n        self.region_aggregator = RegionAggregationFeature(\n            grid_feature_dim=grid_feature_dim,\n            num_regions=num_regions,\n            num_clusters=num_clusters\n        )\n        \n        # Alternative aggregation methods\n        if aggregation_method == 'attention':\n            self.attention_aggregator = AttentionAggregator(grid_feature_dim, num_clusters)\n        elif aggregation_method == 'pooling':\n            self.pooling_aggregator = PoolingAggregator(grid_feature_dim, num_clusters)\n    \n    def forward(self, grid_features: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Forward pass with multiple outputs\n        Args:\n            grid_features: [B, M, D] grid features\n        Returns:\n            output_dict: Dictionary containing various aggregated features\n        \"\"\"\n        # Main region aggregation\n        regional_features, correlation_weights = self.region_aggregator(grid_features)\n        \n        output_dict = {\n            'regional_features': regional_features,  # R^(1): [B, C, D]\n            'correlation_weights': correlation_weights,  # r_ji^(0): [B, M, C]\n            'cluster_assignments': self.region_aggregator.get_cluster_assignments(correlation_weights)\n        }\n        \n        # Alternative aggregation methods\n        if self.aggregation_method == 'attention':\n            attention_features = self.attention_aggregator(grid_features)\n            output_dict['attention_features'] = attention_features\n        \n        elif self.aggregation_method == 'pooling':\n            pooled_features = self.pooling_aggregator(grid_features)\n            output_dict['pooled_features'] = pooled_features\n        \n        return output_dict\n\nclass AttentionAggregator(nn.Module):\n    \"\"\"Alternative attention-based aggregation\"\"\"\n    def __init__(self, feature_dim: int, num_regions: int):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(feature_dim, num_heads=8, batch_first=True)\n        self.region_queries = nn.Parameter(torch.randn(num_regions, feature_dim))\n    \n    def forward(self, grid_features: torch.Tensor) -> torch.Tensor:\n        B, M, D = grid_features.shape\n        \n        # Expand region queries for batch\n        queries = self.region_queries.unsqueeze(0).expand(B, -1, -1)  # [B, num_regions, D]\n        \n        # Apply attention\n        aggregated_features, _ = self.attention(queries, grid_features, grid_features)\n        \n        return aggregated_features\n\nclass PoolingAggregator(nn.Module):\n    \"\"\"Alternative pooling-based aggregation\"\"\"\n    def __init__(self, feature_dim: int, num_regions: int):\n        super().__init__()\n        self.num_regions = num_regions\n        self.conv1d = nn.Conv1d(feature_dim, num_regions, kernel_size=1)\n    \n    def forward(self, grid_features: torch.Tensor) -> torch.Tensor:\n        B, M, D = grid_features.shape\n        \n        # Transpose for conv1d: [B, D, M]\n        features_transposed = grid_features.transpose(1, 2)\n        \n        # Apply 1D convolution and global average pooling\n        conv_output = self.conv1d(features_transposed)  # [B, num_regions, M]\n        pooled_output = F.adaptive_avg_pool1d(conv_output, 1).squeeze(-1)  # [B, num_regions]\n        \n        # Expand back to feature dimension\n        pooled_features = pooled_output.unsqueeze(-1).expand(-1, -1, D)  # [B, num_regions, D]\n        \n        return pooled_features\n\n# Testing and example usage\ndef test_region_aggregation_feature():\n    \"\"\"Test the Region Aggregation Feature implementation\"\"\"\n    \n    print(\"Testing Region Aggregation Feature Implementation\")\n    print(\"=\" * 60)\n    \n    # Test parameters\n    batch_size = 2\n    num_grid_positions = 64  # 8x8 grid\n    grid_feature_dim = 128\n    num_clusters = 5\n    \n    # Create model\n    model = RegionAggregationModule(\n        grid_feature_dim=grid_feature_dim,\n        num_regions=10,\n        num_clusters=num_clusters,\n        aggregation_method='attention'\n    )\n    \n    # Create dummy grid features (from Section 3.1.1)\n    grid_features = torch.randn(batch_size, num_grid_positions, grid_feature_dim)\n    \n    print(f\"Input grid features shape: {grid_features.shape}\")\n    print(f\"Grid feature dimension: {grid_feature_dim}\")\n    print(f\"Number of clusters: {num_clusters}\")\n    print(f\"Batch size: {batch_size}\")\n    \n    # Forward pass\n    with torch.no_grad():\n        output_dict = model(grid_features)\n    \n    # Display results\n    print(f\"\\nResults:\")\n    print(f\"Regional features shape: {output_dict['regional_features'].shape}\")\n    print(f\"Correlation weights shape: {output_dict['correlation_weights'].shape}\")\n    print(f\"Cluster assignments shape: {output_dict['cluster_assignments'].shape}\")\n    \n    if 'attention_features' in output_dict:\n        print(f\"Attention features shape: {output_dict['attention_features'].shape}\")\n    \n    # Analyze clustering\n    correlation_weights = output_dict['correlation_weights']\n    cluster_assignments = output_dict['cluster_assignments']\n    \n    print(f\"\\nCluster Analysis:\")\n    for b in range(batch_size):\n        for c in range(num_clusters):\n            cluster_size = torch.sum(cluster_assignments[b] == c).item()\n            avg_weight = torch.mean(correlation_weights[b, cluster_assignments[b] == c, c]).item()\n            print(f\"  Batch {b}, Cluster {c}: {cluster_size} positions, avg weight: {avg_weight:.3f}\")\n    \n    # Test individual components\n    print(f\"\\n\" + \"=\"*60)\n    print(\"Testing Individual Components\")\n    print(\"=\"*60)\n    \n    # Test correlation weights computation\n    region_aggregator = model.region_aggregator\n    correlation_weights = region_aggregator.compute_correlation_weights(\n        grid_features, region_aggregator.cluster_centers\n    )\n    print(f\"Correlation weights range: [{correlation_weights.min():.3f}, {correlation_weights.max():.3f}]\")\n    print(f\"Correlation weights sum per position (should be ~1): {correlation_weights.sum(dim=-1)[0, :5]}\")\n    \n    # Test regional features computation\n    regional_features = region_aggregator.compute_regional_features(grid_features, correlation_weights)\n    print(f\"Regional features shape: {regional_features.shape}\")\n    print(f\"Regional features L2 norm: {torch.norm(regional_features, p=2, dim=-1)[0]}\")\n    \n    # Test clustering loss\n    clustering_loss = region_aggregator.compute_clustering_loss(grid_features, correlation_weights)\n    print(f\"Clustering loss: {clustering_loss.item():.4f}\")\n    \n    # Test learnable parameters\n    print(f\"\\nLearnable Parameters:\")\n    print(f\"Cluster centers shape: {region_aggregator.cluster_centers.shape}\")\n    print(f\"b_r parameter: {region_aggregator.b_r.item():.4f}\")\n    print(f\"b_i parameter: {region_aggregator.b_i.item():.4f}\")\n    print(f\"r_tilde parameter norm: {torch.norm(region_aggregator.r_tilde).item():.4f}\")\n\nif __name__ == \"__main__\":\n    test_region_aggregation_feature()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T03:06:53.944199Z","iopub.execute_input":"2025-05-30T03:06:53.945072Z","iopub.status.idle":"2025-05-30T03:06:54.357481Z","shell.execute_reply.started":"2025-05-30T03:06:53.945049Z","shell.execute_reply":"2025-05-30T03:06:54.355786Z"}},"outputs":[{"name":"stdout","text":"Testing Region Aggregation Feature Implementation\n============================================================\nInput grid features shape: torch.Size([2, 64, 128])\nGrid feature dimension: 128\nNumber of clusters: 5\nBatch size: 2\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_113/710267819.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mtest_region_aggregation_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_113/710267819.py\u001b[0m in \u001b[0;36mtest_region_aggregation_feature\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_113/710267819.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, grid_features)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# Main region aggregation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mregional_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_aggregator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         output_dict = {\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_113/710267819.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, grid_features)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Step 2: Compute regional features r_i^(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mregional_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_regional_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mregional_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_113/710267819.py\u001b[0m in \u001b[0;36mcompute_regional_features\u001b[0;34m(self, grid_features, correlation_weights)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Weighted sum: r_ji^(0) * (v_j^(0) - r̃_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mweighted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_expanded\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcentered_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, M, C, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Sum over grid positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (64) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (2) must match the size of tensor b (64) at non-singleton dimension 1","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}