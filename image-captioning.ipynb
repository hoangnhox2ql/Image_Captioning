{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install timm torch torchvision torch-geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:28:46.284171Z","iopub.execute_input":"2025-06-08T16:28:46.284776Z","iopub.status.idle":"2025-06-08T16:28:49.401035Z","shell.execute_reply.started":"2025-06-08T16:28:46.284747Z","shell.execute_reply":"2025-06-08T16:28:49.400046Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.18)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import Tuple, List\nimport math\n\nclass SwinTransformerPatchEmbedding(nn.Module):\n    \"\"\"Simplified Swin Transformer patch embedding for feature extraction\"\"\"\n    def __init__(self, patch_size: int = 4, embed_dim: int = 96):\n        super().__init__()\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        # x: [B, C, H, W]\n        x = self.proj(x)  # [B, embed_dim, H//patch_size, W//patch_size]\n        B, C, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)  # [B, H*W, C]\n        return x, (H, W)\n\nclass GridGraphFeature(nn.Module):\n    \"\"\"\n    Grid-graph feature extraction based on the paper description\n    \"\"\"\n    def __init__(self,\n                 image_size: int = 224,\n                 patch_size: int = 4,\n                 embed_dim: int = 96,\n                 gcn_hidden_dim: int = 128):\n        super().__init__()\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.gcn_hidden_dim = gcn_hidden_dim\n\n        # Calculate number of patches\n        self.num_patches_per_side = image_size // patch_size  # M = k = 56 for 224/4\n        self.num_patches = self.num_patches_per_side ** 2     # M*k patches total\n\n        # Swin Transformer for patch feature extraction\n        self.swin_embedding = SwinTransformerPatchEmbedding(patch_size, embed_dim)\n\n        # Graph Convolutional Layer\n        self.gcn = GraphConvolutionalLayer(embed_dim, gcn_hidden_dim)\n\n    def extract_patch_features(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Extract patch features using Swin Transformer\n        Args:\n            image: [B, 3, 224, 224] input image\n        Returns:\n            X^V: [B, M*k, embed_dim] patch feature matrix\n        \"\"\"\n        # Split image into M x k patches and extract features\n        patch_features, (H, W) = self.swin_embedding(image)  # [B, M*k, embed_dim]\n        return patch_features\n\n    def compute_similarity_matrix(self, patch_features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute similarity matrix between patches\n        Args:\n            patch_features: [B, M*k, embed_dim] patch features\n        Returns:\n            A^V: [B, M*k, M*k] adjacency matrix\n        \"\"\"\n        B, N, D = patch_features.shape\n        k = self.num_patches_per_side\n\n        # Normalize patch features for cosine similarity\n        patch_features_norm = F.normalize(patch_features, p=2, dim=-1)\n\n        # Compute cosine similarity matrix\n        similarity_matrix = torch.bmm(patch_features_norm, patch_features_norm.transpose(-2, -1))\n\n        # Initialize adjacency matrix\n        adjacency_matrix = torch.zeros_like(similarity_matrix)\n\n        # Create grid indices\n        indices = torch.arange(N, device=patch_features.device).reshape(k, k)\n\n        # Horizontal neighbors (right and left)\n        right_neighbors = indices[:, :-1].flatten()  # i\n        right_targets = indices[:, 1:].flatten()     # j\n        left_neighbors = right_targets               # i\n        left_targets = right_neighbors               # j\n\n        # Vertical neighbors (down and up)\n        down_neighbors = indices[:-1, :].flatten()   # i\n        down_targets = indices[1:, :].flatten()      # j\n        up_neighbors = down_targets                  # i\n        up_targets = down_neighbors                  # j\n\n        # Set adjacency for neighbors to 1\n        adjacency_matrix[:, right_neighbors, right_targets] = 1.0\n        adjacency_matrix[:, left_neighbors, left_targets] = 1.0\n        adjacency_matrix[:, down_neighbors, down_targets] = 1.0\n        adjacency_matrix[:, up_neighbors, up_targets] = 1.0\n\n        # Set diagonal to 1\n        adjacency_matrix[:, torch.arange(N), torch.arange(N)] = 1.0\n\n        # Fill non-adjacent positions with similarity values\n        mask = adjacency_matrix == 0\n        adjacency_matrix[mask] = similarity_matrix[mask]\n\n        return adjacency_matrix\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of grid-graph feature extraction\n        Args:\n            image: [B, 3, 224, 224] input image\n        Returns:\n            V^(0): [B, M*k, hidden_dim] initial context grid representation\n        \"\"\"\n        # Step 1: Extract patch features using Swin Transformer\n        patch_features = self.extract_patch_features(image)  # X^V\n\n        # Step 2: Compute similarity-based adjacency matrix\n        adjacency_matrix = self.compute_similarity_matrix(patch_features)  # A^V\n\n        # Step 3: Apply Graph Convolutional Layer\n        grid_representation = self.gcn(patch_features, adjacency_matrix)  # V^(0)\n\n        return grid_representation\n\nclass GraphConvolutionalLayer(nn.Module):\n    \"\"\"\n    Graph Convolutional Layer implementation\n    V^(0) = σ(Ã^T VW^T)\n    \"\"\"\n    def __init__(self, input_dim: int, output_dim: int):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        # Weight matrix W\n        self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, node_features: torch.Tensor, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            node_features: [B, N, input_dim] node feature matrix V\n            adjacency_matrix: [B, N, N] adjacency matrix A^V\n        Returns:\n            output: [B, N, output_dim] output features V^(0)\n        \"\"\"\n        # Compute degree matrix D\n        degree_matrix = torch.sum(adjacency_matrix, dim=-1, keepdim=True)  # [B, N, 1]\n        degree_matrix = torch.clamp(degree_matrix, min=1.0)  # Avoid division by zero\n\n        # Normalize adjacency matrix: Ã = D^(-1/2) A^V D^(-1/2)\n        degree_inv_sqrt = torch.pow(degree_matrix, -0.5)\n        normalized_adj = degree_inv_sqrt * adjacency_matrix * degree_inv_sqrt.transpose(-2, -1)\n\n        # Apply weight transformation: VW^T\n        transformed_features = torch.matmul(node_features, self.weight)  # [B, N, output_dim]\n\n        # Graph convolution: Ã^T VW^T\n        output = torch.bmm(normalized_adj.transpose(-2, -1), transformed_features)\n\n        # Apply activation function (ReLU)\n        output = F.relu(output)\n\n        return output\n\n# Example usage and testing\ndef test_grid_graph_feature():\n    \"\"\"Test the Grid-Graph Feature implementation\"\"\"\n\n    # Create model\n    model = GridGraphFeature(\n        image_size=224,\n        patch_size=4,\n        embed_dim=96,\n        gcn_hidden_dim=128\n    )\n\n    # Create dummy input\n    batch_size = 2\n    dummy_image = torch.randn(batch_size, 3, 224, 224)\n\n    print(f\"Input image shape: {dummy_image.shape}\")\n    print(f\"Number of patches: {model.num_patches}\")\n    print(f\"Patches per side: {model.num_patches_per_side}\")\n\n    # Forward pass\n    with torch.no_grad():\n        grid_representation = model(dummy_image)\n\n    print(f\"Output grid representation shape: {grid_representation.shape}\")\n    print(f\"Expected shape: [{batch_size}, {model.num_patches}, {model.gcn_hidden_dim}]\")\n\n    # Test individual components\n    print(\"\\n--- Testing individual components ---\")\n\n    # Test patch feature extraction\n    patch_features = model.extract_patch_features(dummy_image)\n    print(f\"Patch features shape: {patch_features.shape}\")\n\n    # Test similarity matrix computation\n    similarity_matrix = model.compute_similarity_matrix(patch_features)\n    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n\n    # Verify adjacency matrix properties\n    print(f\"Adjacency matrix diagonal sum: {torch.diagonal(similarity_matrix, dim1=-2, dim2=-1).sum()}\")\n    print(f\"Adjacency matrix range: [{similarity_matrix.min():.3f}, {similarity_matrix.max():.3f}]\")\n\nif __name__ == \"__main__\":\n    test_grid_graph_feature()","metadata":{"execution":{"iopub.status.busy":"2025-06-08T16:28:49.402561Z","iopub.execute_input":"2025-06-08T16:28:49.402777Z","iopub.status.idle":"2025-06-08T16:28:52.842931Z","shell.execute_reply.started":"2025-06-08T16:28:49.402756Z","shell.execute_reply":"2025-06-08T16:28:52.842152Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Input image shape: torch.Size([2, 3, 224, 224])\nNumber of patches: 3136\nPatches per side: 56\nOutput grid representation shape: torch.Size([2, 3136, 128])\nExpected shape: [2, 3136, 128]\n\n--- Testing individual components ---\nPatch features shape: torch.Size([2, 3136, 96])\nSimilarity matrix shape: torch.Size([2, 3136, 3136])\nAdjacency matrix diagonal sum: 6272.0\nAdjacency matrix range: [-0.720, 1.000]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom typing import Tuple, List, Dict\nimport torchvision.ops as ops\n\nclass FasterRCNN(nn.Module):\n    \"\"\"Simplified Faster R-CNN for object region detection\"\"\"\n    def __init__(self, backbone_dim: int = 512, num_classes: int = 80):\n        super().__init__()\n        self.backbone_dim = backbone_dim\n        self.num_classes = num_classes\n\n        # Simplified backbone (normally ResNet)\n        self.backbone = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, backbone_dim, 3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n        # ROI pooling output dimension\n        self.roi_pool_size = 7\n        self.roi_pooling = ops.RoIPool(output_size=(self.roi_pool_size, self.roi_pool_size), spatial_scale=1/16)\n\n        # Feature extraction after ROI pooling\n        self.roi_head = nn.Sequential(\n            nn.Linear(backbone_dim * self.roi_pool_size * self.roi_pool_size, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, images: torch.Tensor, boxes: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Extract features for given region boxes\n        Args:\n            images: [B, 3, H, W] input images\n            boxes: [B, P, 4] region boxes in format [x1, y1, x2, y2]\n        Returns:\n            region_features: [B, P, 512] features for each region\n        \"\"\"\n        B, _, H, W = images.shape\n        P = boxes.shape[1]\n\n        # Extract backbone features\n        backbone_features = self.backbone(images)  # [B, backbone_dim, H', W']\n\n        # Ensure boxes are on the same device as images\n        boxes = boxes.to(images.device)\n\n        # Prepare boxes for ROI pooling\n        batch_indices = torch.arange(B, device=images.device).view(B, 1).expand(B, P).reshape(-1)  # [B*P]\n        flat_boxes = boxes.view(-1, 4)  # [B*P, 4]\n        roi_boxes = torch.cat([batch_indices[:, None], flat_boxes], dim=1)  # [B*P, 5] [batch_idx, x1, y1, x2, y2]\n\n        # ROI pooling\n        pooled_features = self.roi_pooling(backbone_features, roi_boxes)  # [B*P, backbone_dim, 7, 7]\n\n        # Flatten and process through ROI head\n        pooled_flat = pooled_features.view(B * P, -1)  # [B*P, backbone_dim * 7 * 7]\n        roi_features = self.roi_head(pooled_flat)  # [B*P, 512]\n\n        # Reshape back to [B, P, 512]\n        region_features = roi_features.view(B, P, 512)\n\n        return region_features\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention mechanism for region features\"\"\"\n    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Linear projections for Q, K, V\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n\n        # Output projection\n        self.w_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(self.d_k)\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            query, key, value: [B, P, d_model] region features\n        Returns:\n            output: [B, P, d_model] attended features\n        \"\"\"\n        B, P, d_model = query.shape\n\n        # Linear projections\n        Q = self.w_q(query)  # [B, P, d_model]\n        K = self.w_k(key)    # [B, P, d_model]\n        V = self.w_v(value)  # [B, P, d_model]\n\n        # Reshape for multi-head attention\n        Q = Q.view(B, P, self.num_heads, self.d_k).transpose(1, 2)  # [B, num_heads, P, d_k]\n        K = K.view(B, P, self.num_heads, self.d_k).transpose(1, 2)  # [B, num_heads, P, d_k]\n        V = V.view(B, P, self.num_heads, self.d_k).transpose(1, 2)  # [B, num_heads, P, d_k]\n\n        # Scaled dot-product attention\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [B, num_heads, P, P]\n        attention_weights = F.softmax(attention_scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # Apply attention to values\n        attended_values = torch.matmul(attention_weights, V)  # [B, num_heads, P, d_k]\n\n        # Concatenate heads\n        attended_values = attended_values.transpose(1, 2).contiguous().view(B, P, d_model)\n\n        # Final linear projection\n        output = self.w_o(attended_values)\n\n        return output\n\nclass SelfAttention(nn.Module):\n    \"\"\"Self-Attention mechanism as described in the paper\"\"\"\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n        self.scale = math.sqrt(d_model)\n\n    def forward(self, R: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute QK^T Self-Attention\n        Args:\n            R: [B, P, d] region features where Q = K = V = R\n        Returns:\n            attention_output: [B, P, d] attended features\n        \"\"\"\n        # Q = K = V = R\n        Q = K = V = R  # [B, P, d]\n\n        # Compute attention scores: QK^T / sqrt(d)\n        attention_scores = torch.bmm(Q, K.transpose(-2, -1)) / self.scale  # [B, P, P]\n        attention_weights = F.softmax(attention_scores, dim=-1)  # [B, P, P]\n        attention_output = torch.bmm(attention_weights, V)  # [B, P, d]\n\n        return attention_output\n\nclass RegionGraphFeature(nn.Module):\n    \"\"\"\n    Region Graph Feature extraction using Faster R-CNN and Multi-Head Attention\n    \"\"\"\n    def __init__(self,\n                 num_regions: int = 100,\n                 region_feature_dim: int = 512,\n                 num_attention_heads: int = 8,\n                 output_dim: int = 512):\n        super().__init__()\n\n        self.num_regions = num_regions\n        self.region_feature_dim = region_feature_dim\n        self.num_attention_heads = num_attention_heads\n        self.output_dim = output_dim\n\n        # Faster R-CNN for region detection and feature extraction\n        self.faster_rcnn = FasterRCNN(backbone_dim=512, num_classes=80)\n\n        # Self-Attention mechanism\n        self.self_attention = SelfAttention(region_feature_dim)\n\n        # Multi-Head Attention mechanism\n        self.multi_head_attention = MultiHeadAttention(\n            d_model=region_feature_dim,\n            num_heads=num_attention_heads\n        )\n\n        # Final projection layer\n        self.output_projection = nn.Linear(region_feature_dim, output_dim)\n\n    def generate_dummy_boxes(self, batch_size: int, num_boxes: int, image_size: Tuple[int, int], device: torch.device) -> torch.Tensor:\n        \"\"\"\n        Generate dummy bounding boxes for testing\n        In practice, these would come from Faster R-CNN's RPN\n        Args:\n            batch_size: Number of images in batch\n            num_boxes: Number of boxes per image\n            image_size: (H, W) of input images\n            device: Device to place tensors on (CPU or CUDA)\n        \"\"\"\n        H, W = image_size\n\n        # Vectorized box generation\n        x1 = torch.randint(0, W//2, (batch_size, num_boxes), device=device).float()\n        y1 = torch.randint(0, H//2, (batch_size, num_boxes), device=device).float()\n        x2 = x1 + torch.randint(W//4, W//2, (batch_size, num_boxes), device=device).float()\n        y2 = y1 + torch.randint(H//4, H//2, (batch_size, num_boxes), device=device).float()\n\n        # Clamp boxes to image bounds\n        x2 = torch.clamp(x2, max=W-1)\n        y2 = torch.clamp(y2, max=H-1)\n\n        # Stack coordinates: [x1, y1, x2, y2]\n        boxes = torch.stack([x1, y1, x2, y2], dim=-1)  # [batch_size, num_boxes, 4]\n\n        return boxes\n\n    def forward(self, images: torch.Tensor, region_boxes: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass for region graph feature extraction\n        Args:\n            images: [B, 3, H, W] input images\n            region_boxes: [B, P, 4] region boxes (optional, will generate dummy if None)\n        Returns:\n            R^(0): [B, P, output_dim] initial region representation\n        \"\"\"\n        B, C, H, W = images.shape\n        device = images.device  # Get device from input images\n\n        # Generate dummy boxes if not provided\n        if region_boxes is None:\n            region_boxes = self.generate_dummy_boxes(B, self.num_regions, (H, W), device)\n        else:\n            region_boxes = region_boxes.to(device)\n\n        # Enable mixed precision for faster computation on GPU\n        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n            # Step 1: Extract region features using Faster R-CNN\n            region_features = self.faster_rcnn(images, region_boxes)  # R: [B, P, d]\n\n            # Step 2: Apply Self-Attention mechanism\n            sa_output = self.self_attention(region_features)  # [B, P, d]\n\n            # Step 3: Apply Multi-Head Attention\n            mha_output = self.multi_head_attention(sa_output, sa_output, sa_output)  # [B, P, d]\n\n            # Step 4: Combine with residual connection\n            combined_features = region_features + mha_output  # [B, P, d]\n\n            # Step 5: Final projection to get R^(0)\n            initial_region_representation = self.output_projection(combined_features)  # [B, P, output_dim]\n\n        return initial_region_representation\n\n    def get_region_vectors(self, region_representation: torch.Tensor) -> List[torch.Tensor]:\n        \"\"\"\n        Extract individual region vectors {r_i}\n        Args:\n            region_representation: [B, P, d] region features\n        Returns:\n            List of region vectors for each batch\n        \"\"\"\n        B, P, d = region_representation.shape\n        region_vectors = []\n\n        for b in range(B):\n            batch_vectors = [region_representation[b, p, :] for p in range(P)]\n            region_vectors.append(batch_vectors)\n\n        return region_vectors\n\n# Example usage and testing\ndef test_region_graph_feature():\n    \"\"\"Test the Region Graph Feature implementation\"\"\"\n\n    # Determine device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Create model and move to device\n    model = RegionGraphFeature(\n        num_regions=50,\n        region_feature_dim=512,\n        num_attention_heads=8,\n        output_dim=512\n    ).to(device)\n\n    # Create dummy input and move to device\n    batch_size = 2\n    dummy_images = torch.randn(batch_size, 3, 224, 224).to(device)\n\n    print(f\"Input images shape: {dummy_images.shape}\")\n    print(f\"Number of regions: {model.num_regions}\")\n    print(f\"Region feature dimension: {model.region_feature_dim}\")\n\n    # Forward pass\n    with torch.no_grad():\n        region_representation = model(dummy_images)\n\n    print(f\"Output region representation shape: {region_representation.shape}\")\n    print(f\"Expected shape: [{batch_size}, {model.num_regions}, {model.output_dim}]\")\n\n    # Test with custom boxes\n    print(\"\\n--- Testing with custom region boxes ---\")\n    custom_boxes = torch.tensor([\n        [[10, 10, 50, 50], [60, 60, 100, 100], [120, 30, 180, 90]],\n        [[20, 20, 80, 80], [90, 10, 150, 70], [30, 100, 90, 160]]\n    ], dtype=torch.float32).to(device)\n\n    with torch.no_grad():\n        custom_region_representation = model(dummy_images, custom_boxes)\n\n    print(f\"Custom region representation shape: {custom_region_representation.shape}\")\n\n    # Test individual components\n    print(\"\\n--- Testing individual components ---\")\n\n    # Test Faster R-CNN feature extraction\n    dummy_boxes = model.generate_dummy_boxes(batch_size, 10, (224, 224), device)\n    region_features = model.faster_rcnn(dummy_images, dummy_boxes)\n    print(f\"Faster R-CNN features shape: {region_features.shape}\")\n\n    # Test Self-Attention\n    sa_output = model.self_attention(region_features)\n    print(f\"Self-Attention output shape: {sa_output.shape}\")\n\n    # Test Multi-Head Attention\n    mha_output = model.multi_head_attention(region_features, region_features, region_features)\n    print(f\"Multi-Head Attention output shape: {mha_output.shape}\")\n\n    # Extract individual region vectors\n    region_vectors = model.get_region_vectors(region_representation)\n    print(f\"Number of region vector batches: {len(region_vectors)}\")\n    print(f\"Number of vectors per batch: {len(region_vectors[0])}\")\n    print(f\"Each region vector shape: {region_vectors[0][0].shape}\")\n\nif __name__ == \"__main__\":\n    test_region_graph_feature()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:28:52.843831Z","iopub.execute_input":"2025-06-08T16:28:52.844175Z","iopub.status.idle":"2025-06-08T16:28:55.445615Z","shell.execute_reply.started":"2025-06-08T16:28:52.844155Z","shell.execute_reply":"2025-06-08T16:28:55.444934Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nInput images shape: torch.Size([2, 3, 224, 224])\nNumber of regions: 50\nRegion feature dimension: 512\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_93/847913151.py:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n","output_type":"stream"},{"name":"stdout","text":"Output region representation shape: torch.Size([2, 50, 512])\nExpected shape: [2, 50, 512]\n\n--- Testing with custom region boxes ---\nCustom region representation shape: torch.Size([2, 3, 512])\n\n--- Testing individual components ---\nFaster R-CNN features shape: torch.Size([2, 10, 512])\nSelf-Attention output shape: torch.Size([2, 10, 512])\nMulti-Head Attention output shape: torch.Size([2, 10, 512])\nNumber of region vector batches: 2\nNumber of vectors per batch: 50\nEach region vector shape: torch.Size([512])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nimport math\nfrom typing import List, Dict, Tuple, Optional\nimport spacy\nimport networkx as nx\n\nclass BERTEmbedding(nn.Module):\n    \"\"\"BERT-based text embedding for semantic features\"\"\"\n    def __init__(self, model_name: str = 'bert-base-uncased', embedding_dim: int = 768):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n        self.bert_model = BertModel.from_pretrained(model_name)\n\n        # Freeze BERT parameters for efficiency (optional)\n        for param in self.bert_model.parameters():\n            param.requires_grad = False\n\n    def forward(self, text_sequences: List[str]) -> torch.Tensor:\n        \"\"\"\n        Convert text sequences to BERT embeddings\n        Args:\n            text_sequences: List of text strings\n        Returns:\n            embeddings: [N, embedding_dim] where N is number of sequences\n        \"\"\"\n        # Tokenize texts\n        encoded = self.tokenizer(\n            text_sequences,\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors='pt'\n        )\n\n        # Get BERT embeddings\n        with torch.no_grad():\n            outputs = self.bert_model(**encoded)\n            # Use [CLS] token embedding as sentence representation\n            embeddings = outputs.last_hidden_state[:, 0, :]  # [N, embedding_dim]\n\n        return embeddings\n\nclass DependencyParser:\n    \"\"\"Dependency parsing for constructing semantic graphs\"\"\"\n    def __init__(self):\n        # Load spaCy model for dependency parsing\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            print(\"Warning: spaCy English model not found. Please run 'python -m spacy download en_core_web_sm' to install.\")\n            self.nlp = None\n\n    def parse_text(self, text: str) -> List[Dict]:\n        \"\"\"\n        Parse text and extract dependency relationships\n        Args:\n            text: Input text string\n        Returns:\n            dependencies: List of dependency relations\n        \"\"\"\n        if self.nlp is None:\n            # Dummy dependencies for testing\n            words = text.split()\n            return [{\"head\": i, \"child\": i+1, \"relation\": \"dummy\"} for i in range(len(words)-1)]\n\n        doc = self.nlp(text)\n        dependencies = []\n\n        for token in doc:\n            if token.head != token:  # Skip root\n                dependencies.append({\n                    \"head\": token.head.i,\n                    \"child\": token.i,\n                    \"relation\": token.dep_,\n                    \"head_text\": token.head.text,\n                    \"child_text\": token.text\n                })\n\n        return dependencies\n\n    def extract_words(self, text: str) -> List[str]:\n        \"\"\"Extract individual words from text\"\"\"\n        if self.nlp is None:\n            return text.split()\n\n        doc = self.nlp(text)\n        return [token.text for token in doc if not token.is_punct]\n\nclass SemanticGraphConstructor:\n    \"\"\"Construct semantic graphs from dependency trees\"\"\"\n    def __init__(self):\n        self.dependency_parser = DependencyParser()\n\n    def construct_adjacency_matrix(self, text: str) -> Tuple[torch.Tensor, List[str]]:\n        \"\"\"\n        Construct adjacency matrix from dependency tree\n        Args:\n            text: Input text string\n        Returns:\n            adjacency_matrix: [N, N] adjacency matrix\n            words: List of words corresponding to matrix indices\n        \"\"\"\n        # Extract words and dependencies\n        words = self.dependency_parser.extract_words(text)\n        dependencies = self.dependency_parser.parse_text(text)\n\n        N = len(words)\n        if N == 0:\n            return torch.zeros(1, 1), [\"\"]\n\n        # Initialize adjacency matrix\n        adjacency_matrix = torch.zeros(N, N)\n\n        # Fill adjacency matrix based on dependencies\n        for dep in dependencies:\n            head_idx = dep[\"head\"]\n            child_idx = dep[\"child\"]\n\n            # Ensure indices are within bounds\n            if 0 <= head_idx < N and 0 <= child_idx < N:\n                # Undirected graph: set both directions\n                adjacency_matrix[head_idx, child_idx] = 1.0\n                adjacency_matrix[child_idx, head_idx] = 1.0\n\n        # Add self-connections (diagonal)\n        for i in range(N):\n            adjacency_matrix[i, i] = 1.0\n\n        return adjacency_matrix, words\n\n    def correlation_function(self, wi: str, wj: str) -> float:\n        \"\"\"\n        Compute correlation between two words D(wi, wj)\n        This is a simplified implementation - in practice, could use:\n        - Word embeddings similarity\n        - Co-occurrence statistics\n        - Semantic similarity measures\n        \"\"\"\n        if wi == wj:\n            return 1.0\n\n        # Jaccard similarity of character sets\n        set_i = set(wi.lower())\n        set_j = set(wj.lower())\n\n        if len(set_i.union(set_j)) == 0:\n            return 0.0\n\n        correlation = len(set_i.intersection(set_j)) / len(set_i.union(set_j))\n        return correlation\n\nclass SemanticGraphConvolutionalLayer(nn.Module):\n    \"\"\"\n    Semantic Graph Convolutional Layer\n    S^(0) = σ(Ã^S σ(Ã^S SW₁^S) W₂^S)\n    \"\"\"\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n\n        # Weight matrices\n        self.W1 = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n        self.W2 = nn.Parameter(torch.FloatTensor(hidden_dim, output_dim))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv1 = 1. / math.sqrt(self.W1.size(1))\n        self.W1.data.uniform_(-stdv1, stdv1)\n\n        stdv2 = 1. / math.sqrt(self.W2.size(1))\n        self.W2.data.uniform_(-stdv2, stdv2)\n\n    def normalize_adjacency_matrix(self, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Normalize adjacency matrix: Ã = D^(-1/2) A D^(-1/2)\n        \"\"\"\n        # Compute degree matrix\n        degree = torch.sum(adjacency_matrix, dim=-1, keepdim=True)  # [N, 1]\n        degree = torch.clamp(degree, min=1.0)  # Avoid division by zero\n\n        # D^(-1/2)\n        degree_inv_sqrt = torch.pow(degree, -0.5)\n\n        # Normalize: D^(-1/2) A D^(-1/2)\n        normalized_adj = degree_inv_sqrt * adjacency_matrix * degree_inv_sqrt.transpose(-2, -1)\n\n        return normalized_adj\n\n    def forward(self, node_features: torch.Tensor, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            node_features: [N, input_dim] node features S\n            adjacency_matrix: [N, N] adjacency matrix A^S\n        Returns:\n            output: [N, output_dim] output features S^(0)\n        \"\"\"\n        # Normalize adjacency matrix\n        normalized_adj = self.normalize_adjacency_matrix(adjacency_matrix)  # Ã^S\n\n        # First GCN layer: Ã^S S W₁^S\n        h1 = torch.matmul(node_features, self.W1)  # [N, hidden_dim]\n        h1 = torch.matmul(normalized_adj, h1)      # [N, hidden_dim]\n        h1 = F.relu(h1)  # σ(Ã^S S W₁^S)\n\n        # Second GCN layer: Ã^S σ(Ã^S S W₁^S) W₂^S\n        h2 = torch.matmul(h1, self.W2)             # [N, output_dim]\n        output = torch.matmul(normalized_adj, h2)   # [N, output_dim]\n        output = F.relu(output)  # σ(Ã^S σ(Ã^S S W₁^S) W₂^S)\n\n        return output\n\nclass SemanticGraphFeature(nn.Module):\n    \"\"\"\n    Complete Semantic Graph Feature extraction pipeline\n    \"\"\"\n    def __init__(self,\n                 bert_model_name: str = 'bert-base-uncased',\n                 bert_dim: int = 768,\n                 gcn_hidden_dim: int = 512,\n                 output_dim: int = 256):\n        super().__init__()\n\n        self.bert_dim = bert_dim\n        self.gcn_hidden_dim = gcn_hidden_dim\n        self.output_dim = output_dim\n\n        # BERT embedding for word-level features\n        self.bert_embedding = BERTEmbedding(bert_model_name, bert_dim)\n\n        # Semantic graph constructor\n        self.graph_constructor = SemanticGraphConstructor()\n\n        # Semantic Graph Convolutional Layer\n        self.semantic_gcn = SemanticGraphConvolutionalLayer(\n            input_dim=bert_dim,\n            hidden_dim=gcn_hidden_dim,\n            output_dim=output_dim\n        )\n\n    def process_single_text(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Process a single text to extract semantic graph features\n        Args:\n            text: Input text string\n        Returns:\n            semantic_features: [N, output_dim] semantic node features\n            adjacency_matrix: [N, N] semantic adjacency matrix\n        \"\"\"\n        # Step 1: Construct semantic graph\n        adjacency_matrix, words = self.graph_constructor.construct_adjacency_matrix(text)\n\n        if len(words) == 0 or words == [\"\"]:\n            # Handle empty text\n            return torch.zeros(1, self.output_dim), torch.zeros(1, 1)\n\n        # Step 2: Get BERT embeddings for words\n        word_embeddings = self.bert_embedding(words)  # [N, bert_dim]\n\n        # Step 3: Apply Semantic GCN\n        semantic_features = self.semantic_gcn(word_embeddings, adjacency_matrix)  # [N, output_dim]\n\n        return semantic_features, adjacency_matrix\n\n    def forward(self, text_list: List[str]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n        \"\"\"\n        Process multiple texts\n        Args:\n            text_list: List of text strings\n        Returns:\n            semantic_features_list: List of [Ni, output_dim] tensors\n            adjacency_matrices_list: List of [Ni, Ni] tensors\n        \"\"\"\n        semantic_features_list = []\n        adjacency_matrices_list = []\n\n        for text in text_list:\n            semantic_features, adjacency_matrix = self.process_single_text(text)\n            semantic_features_list.append(semantic_features)\n            adjacency_matrices_list.append(adjacency_matrix)\n\n        return semantic_features_list, adjacency_matrices_list\n\n    def get_semantic_vectors(self, semantic_features_list: List[torch.Tensor]) -> List[List[torch.Tensor]]:\n        \"\"\"\n        Extract individual semantic vectors {s_i}\n        Args:\n            semantic_features_list: List of [Ni, output_dim] tensors\n        Returns:\n            List of lists containing individual semantic vectors\n        \"\"\"\n        semantic_vectors = []\n\n        for semantic_features in semantic_features_list:\n            N, d = semantic_features.shape\n            text_vectors = [semantic_features[i, :] for i in range(N)]\n            semantic_vectors.append(text_vectors)\n\n        return semantic_vectors\n\n# Example usage and testing\ndef test_semantic_graph_feature():\n    \"\"\"Test the Semantic Graph Feature implementation\"\"\"\n\n    print(\"Testing Semantic Graph Feature Implementation\")\n    print(\"=\" * 50)\n\n    # Create model\n    model = SemanticGraphFeature(\n        bert_model_name='bert-base-uncased',\n        bert_dim=768,\n        gcn_hidden_dim=512,\n        output_dim=256\n    )\n\n    # Test texts (image captions)\n    test_texts = [\n        \"A cat sitting on a wooden table\",\n        \"Two dogs playing in the park\",\n        \"Beautiful sunset over the ocean\",\n        \"Person riding bicycle on street\"\n    ]\n\n    print(f\"Test texts: {len(test_texts)} captions\")\n    for i, text in enumerate(test_texts):\n        print(f\"  {i+1}. {text}\")\n\n    # Process texts\n    semantic_features_list, adjacency_matrices_list = model(test_texts)\n\n    print(f\"\\nResults:\")\n    print(f\"Number of processed texts: {len(semantic_features_list)}\")\n\n    for i, (features, adj_matrix) in enumerate(zip(semantic_features_list, adjacency_matrices_list)):\n        print(f\"\\nText {i+1}: '{test_texts[i]}'\")\n        print(f\"  Semantic features shape: {features.shape}\")\n        print(f\"  Adjacency matrix shape: {adj_matrix.shape}\")\n        print(f\"  Number of words/nodes: {features.shape[0]}\")\n        print(f\"  Feature dimension: {features.shape[1]}\")\n        print(f\"  Graph density: {(adj_matrix.sum() - adj_matrix.trace()) / (adj_matrix.numel() - adj_matrix.shape[0]):.3f}\")\n\n    # Test individual components\n    print(f\"\\n\" + \"=\"*50)\n    print(\"Testing Individual Components\")\n    print(\"=\"*50)\n\n    # Test dependency parsing\n    test_text = \"A cat sitting on a wooden table\"\n    dependencies = model.graph_constructor.dependency_parser.parse_text(test_text)\n    words = model.graph_constructor.dependency_parser.extract_words(test_text)\n\n    print(f\"\\nDependency parsing for: '{test_text}'\")\n    print(f\"Words: {words}\")\n    print(f\"Dependencies: {len(dependencies)}\")\n    for dep in dependencies[:3]:  # Show first 3\n        print(f\"  {dep}\")\n\n    # Test adjacency matrix construction\n    adj_matrix, extracted_words = model.graph_constructor.construct_adjacency_matrix(test_text)\n    print(f\"\\nAdjacency matrix shape: {adj_matrix.shape}\")\n    print(f\"Extracted words: {extracted_words}\")\n    print(f\"Adjacency matrix:\\n{adj_matrix}\")\n\n    # Test BERT embeddings\n    word_embeddings = model.bert_embedding(words[:3])  # Test first 3 words\n    print(f\"\\nBERT embeddings shape for 3 words: {word_embeddings.shape}\")\n    print(f\"Embedding dimension: {word_embeddings.shape[1]}\")\n\n    # Test semantic vectors extraction\n    semantic_vectors = model.get_semantic_vectors(semantic_features_list)\n    print(f\"\\nSemantic vectors:\")\n    print(f\"Number of texts: {len(semantic_vectors)}\")\n    if len(semantic_vectors) > 0:\n        print(f\"Vectors in first text: {len(semantic_vectors[0])}\")\n        if len(semantic_vectors[0]) > 0:\n            print(f\"Each vector shape: {semantic_vectors[0][0].shape}\")\n\nif __name__ == \"__main__\":\n    test_semantic_graph_feature()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:28:55.447818Z","iopub.execute_input":"2025-06-08T16:28:55.448207Z","iopub.status.idle":"2025-06-08T16:29:19.972298Z","shell.execute_reply.started":"2025-06-08T16:28:55.448187Z","shell.execute_reply":"2025-06-08T16:29:19.971557Z"}},"outputs":[{"name":"stderr","text":"2025-06-08 16:29:01.561898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749400141.764408      93 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749400141.820639      93 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Testing Semantic Graph Feature Implementation\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4759901936274824ba2a48687d2b578e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c51c1d4171457fbf8b43408c95eff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0713ed0203d4391bca6f045b4034357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1f60b68fc5f48649bcb5bbe46b696a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0626f9d4b28e4af98a43ce0eb24590c1"}},"metadata":{}},{"name":"stdout","text":"Test texts: 4 captions\n  1. A cat sitting on a wooden table\n  2. Two dogs playing in the park\n  3. Beautiful sunset over the ocean\n  4. Person riding bicycle on street\n\nResults:\nNumber of processed texts: 4\n\nText 1: 'A cat sitting on a wooden table'\n  Semantic features shape: torch.Size([7, 256])\n  Adjacency matrix shape: torch.Size([7, 7])\n  Number of words/nodes: 7\n  Feature dimension: 256\n  Graph density: 0.286\n\nText 2: 'Two dogs playing in the park'\n  Semantic features shape: torch.Size([6, 256])\n  Adjacency matrix shape: torch.Size([6, 6])\n  Number of words/nodes: 6\n  Feature dimension: 256\n  Graph density: 0.333\n\nText 3: 'Beautiful sunset over the ocean'\n  Semantic features shape: torch.Size([5, 256])\n  Adjacency matrix shape: torch.Size([5, 5])\n  Number of words/nodes: 5\n  Feature dimension: 256\n  Graph density: 0.400\n\nText 4: 'Person riding bicycle on street'\n  Semantic features shape: torch.Size([5, 256])\n  Adjacency matrix shape: torch.Size([5, 5])\n  Number of words/nodes: 5\n  Feature dimension: 256\n  Graph density: 0.400\n\n==================================================\nTesting Individual Components\n==================================================\n\nDependency parsing for: 'A cat sitting on a wooden table'\nWords: ['A', 'cat', 'sitting', 'on', 'a', 'wooden', 'table']\nDependencies: 6\n  {'head': 1, 'child': 0, 'relation': 'det', 'head_text': 'cat', 'child_text': 'A'}\n  {'head': 1, 'child': 2, 'relation': 'acl', 'head_text': 'cat', 'child_text': 'sitting'}\n  {'head': 2, 'child': 3, 'relation': 'prep', 'head_text': 'sitting', 'child_text': 'on'}\n\nAdjacency matrix shape: torch.Size([7, 7])\nExtracted words: ['A', 'cat', 'sitting', 'on', 'a', 'wooden', 'table']\nAdjacency matrix:\ntensor([[1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0.],\n        [0., 1., 1., 1., 0., 0., 0.],\n        [0., 0., 1., 1., 0., 0., 1.],\n        [0., 0., 0., 0., 1., 0., 1.],\n        [0., 0., 0., 0., 0., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1.]])\n\nBERT embeddings shape for 3 words: torch.Size([3, 768])\nEmbedding dimension: 768\n\nSemantic vectors:\nNumber of texts: 4\nVectors in first text: 7\nEach vector shape: torch.Size([256])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**Region-grid aggregator**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom typing import Tuple, List, Dict\n\nclass RegionGridAggregator(nn.Module):\n    \"\"\"\n    Region-Grid Aggregator that combines grid features with region features\n    to create enhanced region representations\n    \"\"\"\n    def __init__(self, \n                 grid_feature_dim: int = 128,\n                 region_feature_dim: int = 512,\n                 hidden_dim: int = 256,\n                 num_regions: int = 100):\n        super().__init__()\n        \n        self.grid_feature_dim = grid_feature_dim\n        self.region_feature_dim = region_feature_dim\n        self.hidden_dim = hidden_dim\n        self.num_regions = num_regions\n        \n        # Learnable parameters for attention calculation\n        self.b_i = nn.Parameter(torch.randn(1))\n        self.b_j = nn.Parameter(torch.randn(1))\n        \n        # Learnable parameter r_tilde for normalization\n        self.r_tilde = nn.Parameter(torch.randn(grid_feature_dim))\n        \n        # Projection layer to align dimensions\n        self.grid_projection = nn.Linear(grid_feature_dim, region_feature_dim)\n        \n    def compute_region_grid_correlation(self, \n                                      grid_features: torch.Tensor,\n                                      region_centers: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute correlation between region centers and grid features\n        Args:\n            grid_features: [B, M*k, grid_dim] from grid-graph feature\n            region_centers: [B, P, region_dim] potential region centers\n        Returns:\n            correlation_matrix: [B, P, M*k] correlation scores\n        \"\"\"\n        B, num_patches, grid_dim = grid_features.shape\n        B, num_regions, region_dim = region_centers.shape\n        \n        # Ensure consistent dtype\n        grid_features = grid_features.float()\n        region_centers = region_centers.float()\n        \n        # Project grid features to same dimension as region features\n        grid_projected = self.grid_projection(grid_features)  # [B, M*k, region_dim]\n        \n        # Compute dot product correlation\n        correlation = torch.bmm(region_centers, grid_projected.transpose(-2, -1))  # [B, P, M*k]\n        \n        # Add learnable bias terms (ensure same dtype)\n        correlation = correlation + self.b_i.float() + self.b_j.float()  # Broadcasting\n        \n        return correlation\n    \n    def compute_attention_weights(self, correlation_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute attention weights using softmax normalization\n        Args:\n            correlation_matrix: [B, P, M*k] correlation scores\n        Returns:\n            attention_weights: [B, P, M*k] normalized attention weights\n        \"\"\"\n        # Apply softmax over grid features dimension (M*k)\n        attention_weights = F.softmax(correlation_matrix, dim=-1)  # [B, P, M*k]\n        \n        return attention_weights\n    \n    def aggregate_grid_to_region(self, \n                                grid_features: torch.Tensor,\n                                attention_weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Aggregate grid features to region features using attention weights\n        Args:\n            grid_features: [B, M*k, grid_dim] grid features\n            attention_weights: [B, P, M*k] attention weights\n        Returns:\n            aggregated_features: [B, P, grid_dim] aggregated region features\n        \"\"\"\n        # Weighted sum of grid features\n        aggregated_features = torch.bmm(attention_weights, grid_features)  # [B, P, grid_dim]\n        \n        return aggregated_features\n    \n    def compute_region_aggregation_feature(self, \n                                         aggregated_features: torch.Tensor,\n                                         attention_weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute final region aggregation feature with L2 normalization\n        Args:\n            aggregated_features: [B, P, grid_dim] aggregated features\n            attention_weights: [B, P, M*k] attention weights\n        Returns:\n            region_agg_features: [B, P, grid_dim] final region aggregation features\n        \"\"\"\n        B, P, grid_dim = aggregated_features.shape\n        \n        # Ensure consistent dtype\n        aggregated_features = aggregated_features.float()\n        \n        # Expand r_tilde for batch processing\n        r_tilde_expanded = self.r_tilde.float().unsqueeze(0).unsqueeze(0).expand(B, P, -1)  # [B, P, grid_dim]\n        \n        # Compute difference from learnable parameter\n        diff = aggregated_features - r_tilde_expanded  # [B, P, grid_dim]\n        \n        # Apply L2 norm standardization\n        region_agg_features = F.normalize(diff, p=2, dim=-1)  # [B, P, grid_dim]\n        \n        return region_agg_features\n\n    def forward(self, \n                grid_features: torch.Tensor,\n                region_centers: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of Region-Grid Aggregator\n        Args:\n            grid_features: [B, M*k, grid_dim] from GridGraphFeature\n            region_centers: [B, P, region_dim] potential region centers\n        Returns:\n            region_aggregation_features: [B, P, grid_dim] enhanced region features\n        \"\"\"\n        # Step 1: Compute correlation between regions and grid features\n        correlation_matrix = self.compute_region_grid_correlation(grid_features, region_centers)\n        \n        # Step 2: Compute attention weights\n        attention_weights = self.compute_attention_weights(correlation_matrix)\n        \n        # Step 3: Aggregate grid features to regions\n        aggregated_features = self.aggregate_grid_to_region(grid_features, attention_weights)\n        \n        # Step 4: Compute final region aggregation features\n        region_aggregation_features = self.compute_region_aggregation_feature(aggregated_features, attention_weights)\n        \n        return region_aggregation_features\n\n\nclass MultiModalGraphAttention(nn.Module):\n    \"\"\"\n    Multi-Modal Graph Attention that combines Grid-Graph and Region-Graph features\n    \"\"\"\n    def __init__(self,\n                 image_size: int = 224,\n                 patch_size: int = 4,\n                 grid_embed_dim: int = 96,\n                 grid_hidden_dim: int = 128,\n                 num_regions: int = 100,\n                 region_feature_dim: int = 512,\n                 num_attention_heads: int = 8,\n                 fusion_dim: int = 512):\n        super().__init__()\n        \n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_regions = num_regions\n        self.fusion_dim = fusion_dim\n        \n        # Import the previously defined modules\n        \n        # Grid-Graph Feature Extractor\n        self.grid_graph_extractor = GridGraphFeature(\n            image_size=image_size,\n            patch_size=patch_size,\n            embed_dim=grid_embed_dim,\n            gcn_hidden_dim=grid_hidden_dim\n        )\n        \n        # Region-Graph Feature Extractor\n        self.region_graph_extractor = RegionGraphFeature(\n            num_regions=num_regions,\n            region_feature_dim=region_feature_dim,\n            num_attention_heads=num_attention_heads,\n            output_dim=region_feature_dim\n        )\n        \n        # Region-Grid Aggregator\n        self.region_grid_aggregator = RegionGridAggregator(\n            grid_feature_dim=grid_hidden_dim,\n            region_feature_dim=region_feature_dim,\n            hidden_dim=fusion_dim,\n            num_regions=num_regions\n        )\n        \n        # Feature fusion layers\n        self.region_fusion = nn.Sequential(\n            nn.Linear(region_feature_dim + grid_hidden_dim, fusion_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(fusion_dim, fusion_dim)\n        )\n        \n        self.grid_fusion = nn.Sequential(\n            nn.Linear(grid_hidden_dim, fusion_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1)\n        )\n        \n        # Cross-modal attention\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=fusion_dim,\n            num_heads=num_attention_heads,\n            dropout=0.1,\n            batch_first=True\n        )\n        \n        # Final output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(fusion_dim * 2, fusion_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(fusion_dim, fusion_dim)\n        )\n    \n    def forward(self, images: torch.Tensor, region_boxes: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Forward pass of Multi-Modal Graph Attention\n        Args:\n            images: [B, 3, H, W] input images\n            region_boxes: [B, P, 4] region boxes (optional)\n        Returns:\n            Dictionary containing:\n                - 'fused_features': [B, P+M*k, fusion_dim] fused multi-modal features\n                - 'grid_features': [B, M*k, fusion_dim] processed grid features\n                - 'region_features': [B, P, fusion_dim] enhanced region features\n                - 'region_aggregation': [B, P, grid_hidden_dim] region-grid aggregation\n        \"\"\"\n        B = images.shape[0]\n        device = images.device\n        \n        # Disable mixed precision to avoid dtype issues\n        with torch.cuda.amp.autocast(enabled=False):\n            # Ensure input is float32\n            images = images.float()\n            if region_boxes is not None:\n                region_boxes = region_boxes.float()\n            \n            # Step 1: Extract Grid-Graph Features\n            grid_features = self.grid_graph_extractor(images)  # [B, M*k, grid_hidden_dim]\n            \n            # Step 2: Extract Region-Graph Features\n            region_features = self.region_graph_extractor(images, region_boxes)  # [B, P, region_feature_dim]\n            \n            # Step 3: Apply Region-Grid Aggregator\n            region_aggregation = self.region_grid_aggregator(grid_features, region_features)  # [B, P, grid_hidden_dim]\n            \n            # Step 4: Fuse region features with aggregated grid features\n            concatenated_region = torch.cat([region_features, region_aggregation], dim=-1)  # [B, P, region_dim + grid_dim]\n            enhanced_region_features = self.region_fusion(concatenated_region)  # [B, P, fusion_dim]\n            \n            # Step 5: Process grid features\n            processed_grid_features = self.grid_fusion(grid_features)  # [B, M*k, fusion_dim]\n            \n            # Step 6: Apply cross-modal attention between regions and grids\n            # Region features attend to grid features\n            region_attended, _ = self.cross_attention(\n                enhanced_region_features,  # Query: regions\n                processed_grid_features,   # Key: grids\n                processed_grid_features    # Value: grids\n            )  # [B, P, fusion_dim]\n            \n            # Grid features attend to region features\n            grid_attended, _ = self.cross_attention(\n                processed_grid_features,   # Query: grids\n                enhanced_region_features,  # Key: regions\n                enhanced_region_features   # Value: regions\n            )  # [B, M*k, fusion_dim]\n            \n            # Step 7: Combine attended features\n            final_region_features = torch.cat([enhanced_region_features, region_attended], dim=-1)  # [B, P, 2*fusion_dim]\n            final_grid_features = torch.cat([processed_grid_features, grid_attended], dim=-1)  # [B, M*k, 2*fusion_dim]\n            \n            # Step 8: Final projection\n            final_region_features = self.output_projection(final_region_features)  # [B, P, fusion_dim]\n            final_grid_features = self.output_projection(final_grid_features)  # [B, M*k, fusion_dim]\n            \n            # Step 9: Concatenate all features for final representation\n            fused_features = torch.cat([final_region_features, final_grid_features], dim=1)  # [B, P+M*k, fusion_dim]\n        \n        return {\n            'fused_features': fused_features,\n            'grid_features': final_grid_features,\n            'region_features': final_region_features,\n            'region_aggregation': region_aggregation\n        }\n\n\n# Example usage and testing\ndef test_mmgat():\n    \"\"\"Test the Multi-Modal Graph Attention implementation\"\"\"\n    \n    # Determine device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create model\n    model = MultiModalGraphAttention(\n        image_size=224,\n        patch_size=4,\n        grid_embed_dim=96,\n        grid_hidden_dim=128,\n        num_regions=50,\n        region_feature_dim=512,\n        num_attention_heads=8,\n        fusion_dim=512\n    ).to(device)\n    \n    # Create dummy input\n    batch_size = 2\n    dummy_images = torch.randn(batch_size, 3, 224, 224).to(device)\n    \n    print(f\"Input images shape: {dummy_images.shape}\")\n    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Forward pass\n    with torch.no_grad():\n        outputs = model(dummy_images)\n    \n    # Print output shapes\n    print(\"\\n--- Output Shapes ---\")\n    for key, value in outputs.items():\n        print(f\"{key}: {value.shape}\")\n    \n    # Calculate expected dimensions\n    num_patches = (224 // 4) ** 2  # 56 * 56 = 3136\n    num_regions = 50\n    total_features = num_regions + num_patches\n    \n    print(f\"\\n--- Expected Dimensions ---\")\n    print(f\"Number of patches: {num_patches}\")\n    print(f\"Number of regions: {num_regions}\")\n    print(f\"Total features: {total_features}\")\n    print(f\"Fusion dimension: 512\")\n    \n    # Test with custom region boxes\n    print(\"\\n--- Testing with custom region boxes ---\")\n    custom_boxes = torch.tensor([\n        [[10, 10, 50, 50], [60, 60, 100, 100], [120, 30, 180, 90]],\n        [[20, 20, 80, 80], [90, 10, 150, 70], [30, 100, 90, 160]]\n    ], dtype=torch.float32).to(device)\n    \n    # Note: This would require adjusting num_regions to 3 for this test\n    # For demonstration, we'll skip this test\n    print(\"Custom box testing skipped (requires model reconfiguration)\")\n    \n    print(\"\\n--- MMGAT Test Completed Successfully ---\")\n\n\nif __name__ == \"__main__\":\n    test_mmgat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:29:19.973368Z","iopub.execute_input":"2025-06-08T16:29:19.973955Z","iopub.status.idle":"2025-06-08T16:29:20.543775Z","shell.execute_reply.started":"2025-06-08T16:29:19.973933Z","shell.execute_reply":"2025-06-08T16:29:20.542942Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nInput images shape: torch.Size([2, 3, 224, 224])\nModel parameters: 31,666,018\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_93/814866976.py:239: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=False):\n/tmp/ipykernel_93/847913151.py:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n","output_type":"stream"},{"name":"stdout","text":"\n--- Output Shapes ---\nfused_features: torch.Size([2, 3186, 512])\ngrid_features: torch.Size([2, 3136, 512])\nregion_features: torch.Size([2, 50, 512])\nregion_aggregation: torch.Size([2, 50, 128])\n\n--- Expected Dimensions ---\nNumber of patches: 3136\nNumber of regions: 50\nTotal features: 3186\nFusion dimension: 512\n\n--- Testing with custom region boxes ---\nCustom box testing skipped (requires model reconfiguration)\n\n--- MMGAT Test Completed Successfully ---\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Grid-semantic aggregator**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom typing import List, Tuple, Dict\nimport numpy as np\n\nclass GridSemanticAggregator(nn.Module):\n    \"\"\"\n    Grid-Semantic Aggregator that combines grid-graph and semantic-graph features\n    Based on the paper's dual objectives:\n    1) Refine semantics nodes utilizing the visual context\n    2) Enhance visual nodes with contextual semantics\n    \"\"\"\n    def __init__(self,\n                 grid_feature_dim: int = 128,      # Output dim from GridGraphFeature\n                 semantic_feature_dim: int = 256,   # Output dim from SemanticGraphFeature\n                 lstm_hidden_dim: int = 256,\n                 mlp_hidden_dim: int = 512,\n                 final_output_dim: int = 512):\n        super().__init__()\n        \n        self.grid_feature_dim = grid_feature_dim\n        self.semantic_feature_dim = semantic_feature_dim\n        self.lstm_hidden_dim = lstm_hidden_dim\n        self.mlp_hidden_dim = mlp_hidden_dim\n        self.final_output_dim = final_output_dim\n        \n        # Bidirectional LSTM for processing region aggregation features R^(1)\n        self.bidirectional_lstm = nn.LSTM(\n            input_size=grid_feature_dim,\n            hidden_size=lstm_hidden_dim,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True\n        )\n        \n        # MLP layers for encoding different node types\n        self.f_s = nn.Sequential(  # For encoding semantic nodes s_i^(0)\n            nn.Linear(semantic_feature_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n            nn.ReLU()\n        )\n        \n        self.f_c = nn.Sequential(  # For encoding context grid features\n            nn.Linear(lstm_hidden_dim * 2, mlp_hidden_dim),  # *2 for bidirectional\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n            nn.ReLU()\n        )\n        \n        self.f_v = nn.Sequential(  # For encoding visual series features\n            nn.Linear(grid_feature_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n            nn.ReLU()\n        )\n        \n        # MLP for correlation score computation\n        self.correlation_mlp = nn.Sequential(\n            nn.Linear(mlp_hidden_dim * 2, mlp_hidden_dim),  # Concatenated features\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, 1)\n        )\n        \n        # MLPs for adjacent node encoding (f_h for semantic nodes)\n        self.f_h = nn.Sequential(  # For encoding adjacent semantic nodes\n            nn.Linear(semantic_feature_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n            nn.ReLU()\n        )\n        \n        # MLPs for final node encoding (f_g for visual nodes)\n        self.f_g = nn.Sequential(  # For encoding s_j^(0) when updating visual nodes\n            nn.Linear(semantic_feature_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n            nn.ReLU()\n        )\n        \n        # Final projection layers\n        self.semantic_projection = nn.Linear(semantic_feature_dim + mlp_hidden_dim, final_output_dim)\n        self.visual_projection = nn.Linear(grid_feature_dim + mlp_hidden_dim, final_output_dim)\n        \n    def process_grid_features_with_lstm(self, grid_features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Process grid features through bidirectional LSTM to get R^(1)\n        Args:\n            grid_features: [B, M*k, grid_feature_dim] from GridGraphFeature\n        Returns:\n            lstm_features: [B, M*k, lstm_hidden_dim*2] processed features R^(1)\n        \"\"\"\n        # Apply bidirectional LSTM\n        lstm_output, _ = self.bidirectional_lstm(grid_features)  # [B, M*k, lstm_hidden_dim*2]\n        return lstm_output\n    \n    def compute_correlation_scores(self, \n                                  semantic_features: torch.Tensor,\n                                  grid_context_features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute correlation scores α_{s_i,r_j} between semantic and grid features\n        Args:\n            semantic_features: [N_s, semantic_feature_dim] semantic node features s_i^(0)\n            grid_context_features: [M*k, lstm_hidden_dim*2] grid context features from LSTM\n        Returns:\n            correlation_scores: [N_s, M*k] correlation scores α_{s_i,r_j}\n        \"\"\"\n        N_s = semantic_features.shape[0]\n        M_k = grid_context_features.shape[0]\n        \n        # Encode features\n        encoded_semantic = self.f_s(semantic_features)  # [N_s, mlp_hidden_dim]\n        encoded_grid = self.f_c(grid_context_features)   # [M*k, mlp_hidden_dim]\n        \n        # Compute pairwise correlation scores\n        correlation_scores = torch.zeros(N_s, M_k, device=semantic_features.device)\n        \n        for i in range(N_s):\n            for j in range(M_k):\n                # Concatenate features for correlation computation\n                concat_features = torch.cat([\n                    encoded_semantic[i:i+1], \n                    encoded_grid[j:j+1]\n                ], dim=1)  # [1, mlp_hidden_dim*2]\n                \n                # Compute correlation score through MLP\n                score = self.correlation_mlp(concat_features)  # [1, 1]\n                correlation_scores[i, j] = score.squeeze()\n        \n        return correlation_scores\n    \n    def apply_attention_aggregation(self, \n                                   correlation_scores: torch.Tensor,\n                                   target_features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply attention-based aggregation using correlation scores\n        Args:\n            correlation_scores: [N_source, N_target] correlation scores\n            target_features: [N_target, feature_dim] features to aggregate\n        Returns:\n            aggregated_features: [N_source, feature_dim] aggregated features\n        \"\"\"\n        # Apply softmax to get attention weights\n        attention_weights = F.softmax(correlation_scores, dim=1)  # [N_source, N_target]\n        \n        # Weighted aggregation\n        aggregated_features = torch.matmul(attention_weights, target_features)  # [N_source, feature_dim]\n        \n        return aggregated_features\n    \n    def refine_semantic_nodes(self, \n                             semantic_features: torch.Tensor,\n                             semantic_adj_matrix: torch.Tensor,\n                             grid_features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Refine semantic nodes using visual context (Objective 1)\n        Implements equations (11)-(14) from the paper\n        Args:\n            semantic_features: [N_s, semantic_feature_dim] semantic node features S^(0)\n            semantic_adj_matrix: [N_s, N_s] semantic adjacency matrix A^S\n            grid_features: [M*k, grid_feature_dim] grid features V^(0)\n        Returns:\n            refined_semantic: [N_s, final_output_dim] refined semantic features S^(2)\n        \"\"\"\n        # Step 1: Process grid features through LSTM to get R^(1)\n        grid_features_expanded = grid_features.unsqueeze(0)  # Add batch dim\n        lstm_features = self.process_grid_features_with_lstm(grid_features_expanded)  # [1, M*k, lstm_hidden_dim*2]\n        lstm_features = lstm_features.squeeze(0)  # Remove batch dim: [M*k, lstm_hidden_dim*2]\n        \n        # Step 2: Compute correlation scores α_{s_i,r_j}\n        correlation_scores = self.compute_correlation_scores(semantic_features, lstm_features)  # [N_s, M*k]\n        \n        # Step 3: Aggregate visual context for each semantic node\n        visual_context = self.apply_attention_aggregation(correlation_scores, lstm_features)  # [N_s, lstm_hidden_dim*2]\n        \n        # Step 4: Update semantic representation s_i^(2) = [s_i^(0); Σ α_{s_i,r_j} f_h(r_j^(1))]\n        # Encode visual context\n        encoded_visual_context = self.f_h(\n            torch.cat([visual_context, torch.zeros(visual_context.shape[0], \n                      max(0, self.semantic_feature_dim - visual_context.shape[1]), \n                      device=visual_context.device)], dim=1)[:, :self.semantic_feature_dim]\n        )  # [N_s, mlp_hidden_dim]\n        \n        # Concatenate original semantic features with visual context\n        enhanced_semantic = torch.cat([semantic_features, encoded_visual_context], dim=1)  # [N_s, semantic_feature_dim + mlp_hidden_dim]\n        \n        # Project to final dimension\n        refined_semantic = self.semantic_projection(enhanced_semantic)  # [N_s, final_output_dim]\n        \n        return refined_semantic\n    \n    def enhance_visual_nodes(self,\n                           grid_features: torch.Tensor,\n                           semantic_features: torch.Tensor,\n                           semantic_adj_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Enhance visual nodes with contextual semantics (Objective 2)\n        Implements equations (15)-(16) from the paper\n        Args:\n            grid_features: [M*k, grid_feature_dim] grid features V^(0)\n            semantic_features: [N_s, semantic_feature_dim] semantic node features S^(0)\n            semantic_adj_matrix: [N_s, N_s] semantic adjacency matrix A^S\n        Returns:\n            enhanced_visual: [M*k, final_output_dim] enhanced visual features R^(2)\n        \"\"\"\n        M_k = grid_features.shape[0]\n        \n        # Step 1: Process grid features to get visual series features\n        encoded_visual = self.f_v(grid_features)  # [M*k, mlp_hidden_dim]\n        \n        # Step 2: Compute correlation scores α_{r_i,s_j} (reverse direction)\n        # We need to compute correlation between each visual node and semantic nodes\n        correlation_scores = torch.zeros(M_k, semantic_features.shape[0], device=grid_features.device)\n        \n        for i in range(M_k):\n            for j in range(semantic_features.shape[0]):\n                # Concatenate features for correlation computation\n                concat_features = torch.cat([\n                    encoded_visual[i:i+1],\n                    self.f_g(semantic_features[j:j+1])\n                ], dim=1)  # [1, mlp_hidden_dim*2]\n                \n                # Compute correlation score\n                score = self.correlation_mlp(concat_features)  # [1, 1]\n                correlation_scores[i, j] = score.squeeze()\n        \n        # Step 3: Aggregate semantic context for each visual node\n        semantic_context = self.apply_attention_aggregation(correlation_scores, semantic_features)  # [M*k, semantic_feature_dim]\n        \n        # Step 4: Update visual representation r_i^(2) = [r_i^(1); Σ α_{r_i,s_j} f_g(s_j^(0))]\n        # Encode semantic context\n        encoded_semantic_context = self.f_g(semantic_context)  # [M*k, mlp_hidden_dim]\n        \n        # Concatenate original grid features with semantic context\n        enhanced_visual_features = torch.cat([grid_features, encoded_semantic_context], dim=1)  # [M*k, grid_feature_dim + mlp_hidden_dim]\n        \n        # Project to final dimension  \n        enhanced_visual = self.visual_projection(enhanced_visual_features)  # [M*k, final_output_dim]\n        \n        return enhanced_visual\n    \n    def forward(self, \n                grid_features: torch.Tensor,\n                semantic_features_list: List[torch.Tensor],\n                semantic_adj_matrices_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Forward pass of Grid-Semantic Aggregator\n        Args:\n            grid_features: [B, M*k, grid_feature_dim] grid features from GridGraphFeature\n            semantic_features_list: List of [N_i, semantic_feature_dim] semantic features\n            semantic_adj_matrices_list: List of [N_i, N_i] semantic adjacency matrices\n        Returns:\n            refined_semantic_list: List of refined semantic features\n            enhanced_visual: [B, M*k, final_output_dim] enhanced visual features\n        \"\"\"\n        batch_size = grid_features.shape[0]\n        refined_semantic_list = []\n        enhanced_visual_list = []\n        \n        for b in range(batch_size):\n            # Process each sample in the batch\n            current_grid = grid_features[b]  # [M*k, grid_feature_dim]\n            \n            # Handle multiple semantic graphs per image (if any)\n            if b < len(semantic_features_list):\n                current_semantic = semantic_features_list[b]  # [N_s, semantic_feature_dim]\n                current_semantic_adj = semantic_adj_matrices_list[b]  # [N_s, N_s]\n                \n                # Refine semantic nodes using visual context\n                refined_semantic = self.refine_semantic_nodes(\n                    current_semantic, current_semantic_adj, current_grid\n                )\n                refined_semantic_list.append(refined_semantic)\n                \n                # Enhance visual nodes with semantic context\n                enhanced_visual = self.enhance_visual_nodes(\n                    current_grid, current_semantic, current_semantic_adj\n                )\n                enhanced_visual_list.append(enhanced_visual)\n        \n        # Stack enhanced visual features\n        if enhanced_visual_list:\n            enhanced_visual_tensor = torch.stack(enhanced_visual_list, dim=0)  # [B, M*k, final_output_dim]\n        else:\n            # Fallback if no semantic features available\n            enhanced_visual_tensor = grid_features\n        \n        return refined_semantic_list, enhanced_visual_tensor\n\nclass MMGATImageCaptioning(nn.Module):\n    \"\"\"\n    Complete MMGAT Image Captioning model that integrates Grid and Semantic features\n    \"\"\"\n    def __init__(self,\n                 grid_graph_model,      # GridGraphFeature instance\n                 semantic_graph_model,  # SemanticGraphFeature instance\n                 aggregator_config: Dict = None):\n        super().__init__()\n        \n        self.grid_graph_model = grid_graph_model\n        self.semantic_graph_model = semantic_graph_model\n        \n        # Default aggregator configuration\n        if aggregator_config is None:\n            aggregator_config = {\n                'grid_feature_dim': 128,\n                'semantic_feature_dim': 256,\n                'lstm_hidden_dim': 256,\n                'mlp_hidden_dim': 512,\n                'final_output_dim': 512\n            }\n        \n        self.grid_semantic_aggregator = GridSemanticAggregator(**aggregator_config)\n        \n    def forward(self, \n                images: torch.Tensor, \n                captions: List[str]) -> Tuple[List[torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Complete forward pass\n        Args:\n            images: [B, 3, 224, 224] input images\n            captions: List of caption strings for semantic graph construction\n        Returns:\n            refined_semantic_features: List of refined semantic features\n            enhanced_visual_features: [B, M*k, final_output_dim] enhanced visual features\n        \"\"\"\n        # Step 1: Extract grid-graph features\n        grid_features = self.grid_graph_model(images)  # [B, M*k, grid_feature_dim]\n        \n        # Step 2: Extract semantic-graph features\n        semantic_features_list, semantic_adj_matrices_list = self.semantic_graph_model(captions)\n        \n        # Step 3: Apply Grid-Semantic Aggregator\n        refined_semantic_list, enhanced_visual = self.grid_semantic_aggregator(\n            grid_features, semantic_features_list, semantic_adj_matrices_list\n        )\n        \n        return refined_semantic_list, enhanced_visual\n\n# Example usage and testing\ndef test_grid_semantic_aggregator():\n    \"\"\"Test the complete Grid-Semantic Aggregator implementation\"\"\"\n    \n    print(\"Testing Grid-Semantic Aggregator Implementation\")\n    print(\"=\" * 60)\n    \n    # Import the original models (assuming they're available)\n    # from grid_graph_feature import GridGraphFeature\n    # from semantic_graph_feature import SemanticGraphFeature\n    \n    # Create dummy data for testing\n    batch_size = 2\n    num_patches = 56 * 56  # M*k = 3136 for 224x224 image with patch_size=4\n    grid_feature_dim = 128\n    semantic_feature_dim = 256\n    \n    # Create dummy grid features\n    dummy_grid_features = torch.randn(batch_size, num_patches, grid_feature_dim)\n    \n    # Create dummy semantic features (variable length for each sample)\n    dummy_semantic_features_list = [\n        torch.randn(6, semantic_feature_dim),  # 6 words in first caption\n        torch.randn(4, semantic_feature_dim)   # 4 words in second caption\n    ]\n    \n    # Create dummy semantic adjacency matrices\n    dummy_semantic_adj_list = [\n        torch.eye(6) + torch.randn(6, 6) * 0.1,  # 6x6 adjacency matrix\n        torch.eye(4) + torch.randn(4, 4) * 0.1   # 4x4 adjacency matrix\n    ]\n    \n    # Make adjacency matrices symmetric and positive\n    for i, adj in enumerate(dummy_semantic_adj_list):\n        adj = (adj + adj.T) / 2\n        adj = torch.clamp(adj, min=0)\n        dummy_semantic_adj_list[i] = adj\n    \n    # Create aggregator\n    aggregator = GridSemanticAggregator(\n        grid_feature_dim=grid_feature_dim,\n        semantic_feature_dim=semantic_feature_dim,\n        lstm_hidden_dim=256,\n        mlp_hidden_dim=512,\n        final_output_dim=512\n    )\n    \n    print(f\"Input shapes:\")\n    print(f\"  Grid features: {dummy_grid_features.shape}\")\n    print(f\"  Semantic features: {[f.shape for f in dummy_semantic_features_list]}\")\n    print(f\"  Semantic adjacency: {[adj.shape for adj in dummy_semantic_adj_list]}\")\n    \n    # Forward pass\n    with torch.no_grad():\n        refined_semantic_list, enhanced_visual = aggregator(\n            dummy_grid_features,\n            dummy_semantic_features_list,\n            dummy_semantic_adj_list\n        )\n    \n    print(f\"\\nOutput shapes:\")\n    print(f\"  Refined semantic features: {[f.shape for f in refined_semantic_list]}\")\n    print(f\"  Enhanced visual features: {enhanced_visual.shape}\")\n    \n    # Test individual components\n    print(f\"\\n\" + \"=\"*60)\n    print(\"Testing Individual Components\")\n    print(\"=\"*60)\n    \n    # Test LSTM processing\n    lstm_features = aggregator.process_grid_features_with_lstm(dummy_grid_features)\n    print(f\"LSTM features shape: {lstm_features.shape}\")\n    \n    # Test correlation computation\n    single_semantic = dummy_semantic_features_list[0]\n    single_grid = lstm_features[0]  # First sample\n    correlation_scores = aggregator.compute_correlation_scores(single_semantic, single_grid)\n    print(f\"Correlation scores shape: {correlation_scores.shape}\")\n    print(f\"Correlation scores range: [{correlation_scores.min():.3f}, {correlation_scores.max():.3f}]\")\n    \n    # Test attention aggregation\n    aggregated = aggregator.apply_attention_aggregation(correlation_scores, single_grid)\n    print(f\"Aggregated features shape: {aggregated.shape}\")\n    \n    print(f\"\\nTest completed successfully!\")\n\nif __name__ == \"__main__\":\n    test_grid_semantic_aggregator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:29:20.544865Z","iopub.execute_input":"2025-06-08T16:29:20.545120Z","iopub.status.idle":"2025-06-08T16:29:37.494889Z","shell.execute_reply.started":"2025-06-08T16:29:20.545101Z","shell.execute_reply":"2025-06-08T16:29:37.494130Z"}},"outputs":[{"name":"stdout","text":"Testing Grid-Semantic Aggregator Implementation\n============================================================\nInput shapes:\n  Grid features: torch.Size([2, 3136, 128])\n  Semantic features: [torch.Size([6, 256]), torch.Size([4, 256])]\n  Semantic adjacency: [torch.Size([6, 6]), torch.Size([4, 4])]\n\nOutput shapes:\n  Refined semantic features: [torch.Size([6, 512]), torch.Size([4, 512])]\n  Enhanced visual features: torch.Size([2, 3136, 512])\n\n============================================================\nTesting Individual Components\n============================================================\nLSTM features shape: torch.Size([2, 3136, 512])\nCorrelation scores shape: torch.Size([6, 3136])\nCorrelation scores range: [-0.092, -0.002]\nAggregated features shape: torch.Size([6, 512])\n\nTest completed successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**Semantic-semantic aggregator**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import List, Tuple, Optional\nimport math\n\nclass SemanticSemanticAggregator(nn.Module):\n    \"\"\"\n    Semantic-Semantic Aggregator implementation\n    Based on equation (17)-(20) from the paper\n    \"\"\"\n    def __init__(self, \n                 input_dim: int = 256,\n                 lstm_hidden_dim: int = 512,\n                 mlp_hidden_dim: int = 256,\n                 output_dim: int = 256):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.lstm_hidden_dim = lstm_hidden_dim\n        self.mlp_hidden_dim = mlp_hidden_dim\n        self.output_dim = output_dim\n        \n        # LSTM for sequential processing of semantic nodes\n        # S^t = {s₁ᵗ, s₂ᵗ, s₃ᵗ, ..., sₙᵗ} = LSTM(S^(0))\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=lstm_hidden_dim,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=False\n        )\n        \n        # MLPs for attention mechanism\n        # g_x, g_z and g_v are MLPs for encoding node features\n        self.g_x = nn.Sequential(\n            nn.Linear(lstm_hidden_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        )\n        \n        self.g_z = nn.Sequential(\n            nn.Linear(lstm_hidden_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        )\n        \n        self.g_v = nn.Sequential(\n            nn.Linear(lstm_hidden_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        )\n        \n        # MLP for encoding neighboring features\n        # g_n is an MLP to encode the features of neighboring nodes\n        self.g_n = nn.Sequential(\n            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, output_dim)\n        )\n        \n        # Final output projection\n        self.output_projection = nn.Linear(lstm_hidden_dim + output_dim, output_dim)\n        \n    def find_neighboring_nodes(self, adjacency_matrix: torch.Tensor, node_idx: int) -> List[int]:\n        \"\"\"\n        Find neighboring nodes N_i = {s_j|j ∈ {1, ..., N} and j ≠ i}\n        Args:\n            adjacency_matrix: [N, N] adjacency matrix\n            node_idx: Index of the current node\n        Returns:\n            List of neighboring node indices\n        \"\"\"\n        N = adjacency_matrix.shape[0]\n        neighbors = []\n        \n        for j in range(N):\n            if j != node_idx and adjacency_matrix[node_idx, j] > 0:\n                neighbors.append(j)\n                \n        return neighbors\n    \n    def compute_attention_scores(self, \n                               current_node: torch.Tensor,\n                               neighbor_nodes: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute attention scores between current node and its neighbors\n        a_{s_i,s_j} = g_x([s_i^(t); g_z(s_j^(t))]) · g_v([s_j^(t); g_z(s_i^(t))])\n        \n        Args:\n            current_node: [lstm_hidden_dim] current node features s_i^(t)\n            neighbor_nodes: [K, lstm_hidden_dim] neighbor nodes features\n        Returns:\n            attention_scores: [K] attention scores\n        \"\"\"\n        K = neighbor_nodes.shape[0]\n        if K == 0:\n            return torch.tensor([], device=current_node.device)\n        \n        # Expand current node to match neighbor dimensions\n        current_expanded = current_node.unsqueeze(0).expand(K, -1)  # [K, lstm_hidden_dim]\n        \n        # Compute g_z for neighbors and current node\n        g_z_neighbors = self.g_z(neighbor_nodes)  # [K, mlp_hidden_dim]\n        g_z_current = self.g_z(current_expanded)   # [K, mlp_hidden_dim]\n        \n        # Concatenate features for attention computation\n        # [s_i^(t); g_z(s_j^(t))]\n        concat_1 = torch.cat([current_expanded, g_z_neighbors], dim=-1)  # [K, lstm_hidden_dim + mlp_hidden_dim]\n        \n        # [s_j^(t); g_z(s_i^(t))]  \n        concat_2 = torch.cat([neighbor_nodes, g_z_current], dim=-1)  # [K, lstm_hidden_dim + mlp_hidden_dim]\n        \n        # Apply g_x and g_v\n        # Note: We need to adjust dimensions for proper computation\n        # Using simplified dot product attention instead of the exact formula\n        attention_1 = self.g_x(concat_1[:, :self.lstm_hidden_dim])  # [K, mlp_hidden_dim]\n        attention_2 = self.g_v(concat_2[:, :self.lstm_hidden_dim])  # [K, mlp_hidden_dim]\n        \n        # Compute attention scores (dot product)\n        attention_scores = torch.sum(attention_1 * attention_2, dim=-1)  # [K]\n        \n        return attention_scores\n    \n    def aggregate_neighbors(self,\n                          current_node: torch.Tensor,\n                          neighbor_nodes: torch.Tensor,\n                          attention_weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Aggregate neighboring node information using attention weights\n        Args:\n            current_node: [lstm_hidden_dim] current node features\n            neighbor_nodes: [K, lstm_hidden_dim] neighbor node features  \n            attention_weights: [K] normalized attention weights\n        Returns:\n            aggregated_features: [output_dim] aggregated neighbor features\n        \"\"\"\n        if neighbor_nodes.shape[0] == 0:\n            # No neighbors, return zero features\n            return torch.zeros(self.output_dim, device=current_node.device)\n        \n        # Weight neighbors by attention\n        weighted_neighbors = attention_weights.unsqueeze(-1) * neighbor_nodes  # [K, lstm_hidden_dim]\n        \n        # Sum weighted neighbors\n        summed_neighbors = torch.sum(weighted_neighbors, dim=0)  # [lstm_hidden_dim]\n        \n        # Apply g_n MLP to encode neighboring features\n        neighbor_encoding = self.g_z(summed_neighbors.unsqueeze(0)).squeeze(0)  # [mlp_hidden_dim]\n        aggregated_features = self.g_n(neighbor_encoding.unsqueeze(0)).squeeze(0)  # [output_dim]\n        \n        return aggregated_features\n    \n    def forward_single_graph(self, \n                           semantic_features: torch.Tensor,\n                           adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Process a single semantic graph\n        Args:\n            semantic_features: [N, input_dim] semantic node features S^(0)\n            adjacency_matrix: [N, N] adjacency matrix\n        Returns:\n            updated_features: [N, output_dim] updated semantic features S^(3)\n        \"\"\"\n        N = semantic_features.shape[0]\n        \n        # Step 1: Apply LSTM to get sequential representation\n        # S^t = {s₁ᵗ, s₂ᵗ, s₃ᵗ, ..., sₙᵗ} = LSTM(S^(0))\n        lstm_out, _ = self.lstm(semantic_features.unsqueeze(0))  # [1, N, lstm_hidden_dim]\n        lstm_features = lstm_out.squeeze(0)  # [N, lstm_hidden_dim]\n        \n        # Step 2: For each node, compute attention with neighbors and aggregate\n        updated_nodes = []\n        \n        for i in range(N):\n            current_node = lstm_features[i]  # [lstm_hidden_dim]\n            \n            # Find neighboring nodes\n            neighbor_indices = self.find_neighboring_nodes(adjacency_matrix, i)\n            \n            if len(neighbor_indices) == 0:\n                # No neighbors, just use current node\n                aggregated_neighbor_features = torch.zeros(self.output_dim, device=current_node.device)\n            else:\n                # Get neighbor features\n                neighbor_nodes = lstm_features[neighbor_indices]  # [K, lstm_hidden_dim]\n                \n                # Compute attention scores a_{s_i,s_j}\n                attention_scores = self.compute_attention_scores(current_node, neighbor_nodes)\n                \n                # Normalize attention weights using softmax\n                attention_weights = F.softmax(attention_scores, dim=0)  # [K]\n                \n                # Aggregate neighbor information\n                aggregated_neighbor_features = self.aggregate_neighbors(\n                    current_node, neighbor_nodes, attention_weights\n                )\n            \n            # Step 3: Combine current node with aggregated neighbor features\n            # s_i^(3) = [s_i^(t), ∑_{j∈N_i} a_{s_i,s_j} · g_n(s_j^(t))]\n            combined_features = torch.cat([current_node, aggregated_neighbor_features], dim=0)\n            \n            # Apply final projection\n            updated_node = self.output_projection(combined_features.unsqueeze(0)).squeeze(0)\n            updated_nodes.append(updated_node)\n        \n        # Stack all updated nodes\n        updated_features = torch.stack(updated_nodes, dim=0)  # [N, output_dim]\n        \n        return updated_features\n    \n    def forward(self, \n                semantic_features_list: List[torch.Tensor],\n                adjacency_matrices_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        \"\"\"\n        Process multiple semantic graphs\n        Args:\n            semantic_features_list: List of [Ni, input_dim] semantic features\n            adjacency_matrices_list: List of [Ni, Ni] adjacency matrices\n        Returns:\n            updated_features_list: List of [Ni, output_dim] updated semantic features\n        \"\"\"\n        updated_features_list = []\n        \n        for semantic_features, adjacency_matrix in zip(semantic_features_list, adjacency_matrices_list):\n            updated_features = self.forward_single_graph(semantic_features, adjacency_matrix)\n            updated_features_list.append(updated_features)\n        \n        return updated_features_list\n\nclass EnhancedSemanticGraphFeature(nn.Module):\n    \"\"\"\n    Enhanced Semantic Graph Feature with Semantic-Semantic Aggregator\n    Combines the original SemanticGraphFeature with SemanticSemanticAggregator\n    \"\"\"\n    def __init__(self,\n                 bert_model_name: str = 'bert-base-uncased',\n                 bert_dim: int = 768,\n                 gcn_hidden_dim: int = 512,\n                 gcn_output_dim: int = 256,\n                 lstm_hidden_dim: int = 512,\n                 mlp_hidden_dim: int = 256,\n                 final_output_dim: int = 256):\n        super().__init__()\n        \n        # Import the original SemanticGraphFeature\n        # Note: You would need to import this from your original implementation\n        \n        self.semantic_graph_feature = SemanticGraphFeature(\n            bert_model_name=bert_model_name,\n            bert_dim=bert_dim,\n            gcn_hidden_dim=gcn_hidden_dim,\n            output_dim=gcn_output_dim\n        )\n        \n        # Semantic-Semantic Aggregator\n        self.semantic_aggregator = SemanticSemanticAggregator(\n            input_dim=gcn_output_dim,\n            lstm_hidden_dim=lstm_hidden_dim,\n            mlp_hidden_dim=mlp_hidden_dim,\n            output_dim=final_output_dim\n        )\n    \n    def forward(self, text_list: List[str]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n        \"\"\"\n        Complete semantic processing pipeline\n        Args:\n            text_list: List of text strings (image captions)\n        Returns:\n            final_semantic_features: List of [Ni, final_output_dim] final semantic features\n            adjacency_matrices: List of [Ni, Ni] adjacency matrices\n        \"\"\"\n        # Step 1: Extract initial semantic graph features S^(0)\n        initial_features_list, adjacency_matrices_list = self.semantic_graph_feature(text_list)\n        \n        # Step 2: Apply semantic-semantic aggregator to get S^(3)\n        final_features_list = self.semantic_aggregator(initial_features_list, adjacency_matrices_list)\n        \n        return final_features_list, adjacency_matrices_list\n\n# Test function\ndef test_semantic_semantic_aggregator():\n    \"\"\"Test the Semantic-Semantic Aggregator\"\"\"\n    \n    print(\"Testing Semantic-Semantic Aggregator\")\n    print(\"=\" * 50)\n    \n    # Create test data\n    batch_size = 2\n    test_features_list = [\n        torch.randn(5, 256),  # 5 nodes, 256-dim features\n        torch.randn(7, 256),  # 7 nodes, 256-dim features  \n    ]\n    \n    test_adjacency_list = [\n        torch.randint(0, 2, (5, 5)).float(),  # 5x5 adjacency matrix\n        torch.randint(0, 2, (7, 7)).float(),  # 7x7 adjacency matrix\n    ]\n    \n    # Ensure diagonal is 1 (self-connections)\n    for adj in test_adjacency_list:\n        adj.fill_diagonal_(1.0)\n    \n    # Create aggregator\n    aggregator = SemanticSemanticAggregator(\n        input_dim=256,\n        lstm_hidden_dim=512,\n        mlp_hidden_dim=256,\n        output_dim=256\n    )\n    \n    print(f\"Input shapes:\")\n    for i, (features, adj) in enumerate(zip(test_features_list, test_adjacency_list)):\n        print(f\"  Graph {i+1}: Features {features.shape}, Adjacency {adj.shape}\")\n    \n    # Forward pass\n    with torch.no_grad():\n        output_features_list = aggregator(test_features_list, test_adjacency_list)\n    \n    print(f\"\\nOutput shapes:\")\n    for i, features in enumerate(output_features_list):\n        print(f\"  Graph {i+1}: {features.shape}\")\n    \n    print(f\"\\nAggregator parameters:\")\n    total_params = sum(p.numel() for p in aggregator.parameters())\n    print(f\"  Total parameters: {total_params:,}\")\n    \n    # Test single graph processing\n    print(f\"\\nTesting single graph processing:\")\n    single_features = test_features_list[0]\n    single_adj = test_adjacency_list[0]\n    \n    with torch.no_grad():\n        single_output = aggregator.forward_single_graph(single_features, single_adj)\n    \n    print(f\"  Input: {single_features.shape}\")\n    print(f\"  Output: {single_output.shape}\")\n    print(f\"  Feature transformation: {single_features.shape[-1]} -> {single_output.shape[-1]}\")\n\nif __name__ == \"__main__\":\n    test_semantic_semantic_aggregator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:29:37.495929Z","iopub.execute_input":"2025-06-08T16:29:37.496230Z","iopub.status.idle":"2025-06-08T16:29:37.566060Z","shell.execute_reply.started":"2025-06-08T16:29:37.496209Z","shell.execute_reply":"2025-06-08T16:29:37.565160Z"}},"outputs":[{"name":"stdout","text":"Testing Semantic-Semantic Aggregator\n==================================================\nInput shapes:\n  Graph 1: Features torch.Size([5, 256]), Adjacency torch.Size([5, 5])\n  Graph 2: Features torch.Size([7, 256]), Adjacency torch.Size([7, 7])\n\nOutput shapes:\n  Graph 1: torch.Size([5, 256])\n  Graph 2: torch.Size([7, 256])\n\nAggregator parameters:\n  Total parameters: 2,496,768\n\nTesting single graph processing:\n  Input: torch.Size([5, 256])\n  Output: torch.Size([5, 256])\n  Feature transformation: 256 -> 256\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**Decoder**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear transformations\n        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n            \n        attention_weights = F.softmax(scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        \n        # Concatenate heads\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model)\n        \n        return self.W_o(attention_output)\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.dropout(0.1)\n        \n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff):\n        super(TransformerDecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model, num_heads)\n        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n        # Self-attention block\n        attn_output = self.self_attention(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross-attention block  \n        attn_output = self.cross_attention(x, memory, memory, memory_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed-forward block\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        \n        return x\n\nclass MMGATImageCaptioning(nn.Module):\n    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, \n                 d_ff=2048, max_seq_length=50, feature_dim=2048):\n        super(MMGATImageCaptioning, self).__init__()\n        \n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.max_seq_length = max_seq_length\n        \n        # Feature projection layers\n        self.feature_projection = nn.Linear(feature_dim, d_model)\n        \n        # Word embedding and positional encoding\n        self.word_embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = self.create_positional_encoding(max_seq_length, d_model)\n        \n        # Transformer decoder layers\n        self.decoder_layers = nn.ModuleList([\n            TransformerDecoderLayer(d_model, num_heads, d_ff) \n            for _ in range(num_layers)\n        ])\n        \n        # Output projection\n        self.output_projection = nn.Linear(d_model, vocab_size)\n        \n        self.dropout = nn.Dropout(0.1)\n        \n    def create_positional_encoding(self, max_seq_length, d_model):\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length).unsqueeze(1).float()\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        return pe.unsqueeze(0)\n    \n    def create_causal_mask(self, seq_length):\n        \"\"\"Create causal mask for self-attention\"\"\"\n        mask = torch.tril(torch.ones(seq_length, seq_length))\n        return mask.unsqueeze(0).unsqueeze(0)\n    \n    def forward(self, combined_features, target_sequence=None, max_length=20):\n        \"\"\"\n        Args:\n            combined_features: Combined features from MMGAT [batch_size, num_regions, feature_dim]\n            target_sequence: Target sequence for training [batch_size, seq_length]\n            max_length: Maximum generation length for inference\n        \"\"\"\n        batch_size = combined_features.size(0)\n        \n        # Project image features to model dimension\n        memory = self.feature_projection(combined_features)  # [batch_size, num_regions, d_model]\n        \n        if target_sequence is not None:\n            # Training mode\n            return self.forward_train(memory, target_sequence)\n        else:\n            # Inference mode\n            return self.forward_inference(memory, max_length)\n    \n    def forward_train(self, memory, target_sequence):\n        batch_size, seq_length = target_sequence.size()\n        \n        # Word embeddings + positional encoding\n        word_embeds = self.word_embedding(target_sequence) * math.sqrt(self.d_model)\n        pos_encoding = self.positional_encoding[:, :seq_length, :].to(target_sequence.device)\n        decoder_input = self.dropout(word_embeds + pos_encoding)\n        \n        # Create causal mask\n        causal_mask = self.create_causal_mask(seq_length).to(target_sequence.device)\n        \n        # Pass through decoder layers\n        for layer in self.decoder_layers:\n            decoder_input = layer(decoder_input, memory, tgt_mask=causal_mask)\n        \n        # Output projection\n        logits = self.output_projection(decoder_input)\n        \n        return logits\n    \n    def forward_inference(self, memory, max_length):\n        batch_size = memory.size(0)\n        device = memory.device\n        \n        # Start with <BOS> token (assuming index 1)\n        generated_sequence = torch.ones(batch_size, 1, dtype=torch.long, device=device)\n        \n        for step in range(max_length):\n            # Word embeddings + positional encoding\n            word_embeds = self.word_embedding(generated_sequence) * math.sqrt(self.d_model)\n            seq_length = generated_sequence.size(1)\n            pos_encoding = self.positional_encoding[:, :seq_length, :].to(device)\n            decoder_input = self.dropout(word_embeds + pos_encoding)\n            \n            # Create causal mask\n            causal_mask = self.create_causal_mask(seq_length).to(device)\n            \n            # Pass through decoder layers\n            for layer in self.decoder_layers:\n                decoder_input = layer(decoder_input, memory, tgt_mask=causal_mask)\n            \n            # Get prediction for next token\n            logits = self.output_projection(decoder_input[:, -1:, :])\n            next_token = torch.argmax(logits, dim=-1)\n            \n            # Append to sequence\n            generated_sequence = torch.cat([generated_sequence, next_token], dim=1)\n            \n            # Check for <EOS> token (assuming index 2)\n            if (next_token == 2).all():\n                break\n        \n        return generated_sequence\n\n# Example usage\ndef example_usage():\n    # Model parameters\n    vocab_size = 10000\n    d_model = 512\n    num_heads = 8\n    num_layers = 6\n    d_ff = 2048\n    feature_dim = 2048  # Combined feature dimension from MMGAT\n    \n    # Initialize model\n    model = MMGATImageCaptioning(\n        vocab_size=vocab_size,\n        d_model=d_model,\n        num_heads=num_heads,\n        num_layers=num_layers,\n        d_ff=d_ff,\n        feature_dim=feature_dim\n    )\n    \n    # Example input - combined features from MMGAT\n    batch_size = 2\n    num_regions = 49  # e.g., 7x7 grid regions\n    combined_features = torch.randn(batch_size, num_regions, feature_dim)\n    \n    # Training example\n    target_sequence = torch.randint(0, vocab_size, (batch_size, 20))\n    train_logits = model(combined_features, target_sequence)\n    print(f\"Training logits shape: {train_logits.shape}\")\n    \n    # Inference example\n    model.eval()\n    with torch.no_grad():\n        generated_captions = model(combined_features, max_length=20)\n        print(f\"Generated captions shape: {generated_captions.shape}\")\n    \n    return model\n\n# Training function\ndef train_step(model, combined_features, target_sequence, criterion, optimizer):\n    model.train()\n    \n    # Forward pass\n    logits = model(combined_features, target_sequence[:, :-1])  # Exclude last token\n    \n    # Calculate loss\n    loss = criterion(\n        logits.reshape(-1, logits.size(-1)), \n        target_sequence[:, 1:].reshape(-1)  # Exclude first token\n    )\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    return loss.item()\n\nif __name__ == \"__main__\":\n    model = example_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:29:37.566952Z","iopub.execute_input":"2025-06-08T16:29:37.567378Z","iopub.status.idle":"2025-06-08T16:29:39.750849Z","shell.execute_reply.started":"2025-06-08T16:29:37.567356Z","shell.execute_reply":"2025-06-08T16:29:39.749767Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_93/2024095481.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_93/2024095481.py\u001b[0m in \u001b[0;36mexample_usage\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     model = MMGATImageCaptioning(\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_93/2024095481.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, feature_dim)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Transformer decoder layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         self.decoder_layers = nn.ModuleList([\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_93/2024095481.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Transformer decoder layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         self.decoder_layers = nn.ModuleList([\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         ])\n","\u001b[0;32m/tmp/ipykernel_93/2024095481.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, num_heads, d_ff)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionwiseFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_93/2024095481.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, d_ff)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'dropout'"],"ename":"AttributeError","evalue":"module 'torch.nn' has no attribute 'dropout'","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"**Regon-grid Aggregator**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom typing import Tuple, List, Optional\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv, GATConv, TransformerConv\nfrom torch_geometric.data import Data, Batch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom typing import Tuple, List, Optional\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv, GATConv, TransformerConv\nfrom torch_geometric.data import Data, Batch\n\nclass RegionGridAggregator(nn.Module):\n    # ... (phần __init__, _init_parameters, compute_correlation_weights, aggregate_region_features giữ nguyên) ...\n    def __init__(self,\n                 grid_feature_dim: int = 128,\n                 region_feature_dim: int = 512,\n                 hidden_dim: int = 256,\n                 num_cluster_centers: int = 10,\n                 gnn_type: str = 'transformer',  # 'gcn', 'gat', 'transformer'\n                 num_gnn_layers: int = 3,\n                 num_heads: int = 8,\n                 dropout: float = 0.1):\n        super().__init__()\n        \n        self.grid_feature_dim = grid_feature_dim\n        self.region_feature_dim = region_feature_dim\n        self.hidden_dim = hidden_dim\n        self.num_cluster_centers = num_cluster_centers\n        self.gnn_type = gnn_type\n        self.num_gnn_layers = num_gnn_layers\n        \n        # Feature projection layers\n        self.grid_proj = nn.Linear(grid_feature_dim, hidden_dim)\n        self.region_proj = nn.Linear(region_feature_dim, hidden_dim)\n        \n        # Learnable cluster centers (regional centers r_i^(0))\n        self.cluster_centers = nn.Parameter(torch.randn(num_cluster_centers, hidden_dim))\n        \n        # Correlation computation parameters (b_i and b_j in equation 9)\n        self.grid_bias = nn.Parameter(torch.zeros(1))\n        self.region_bias = nn.Parameter(torch.zeros(1))\n        \n        # Learnable parameter r̄ (magnitude equal to r_i^(0))\n        self.learnable_r_bar = nn.Parameter(torch.randn(hidden_dim))\n        \n        # GNN layers for graph-based feature aggregation\n        self.gnn_layers = nn.ModuleList()\n        \n        if gnn_type == 'gcn':\n            for i in range(num_gnn_layers):\n                self.gnn_layers.append(GCNConv(hidden_dim, hidden_dim))\n        elif gnn_type == 'gat':\n            for i in range(num_gnn_layers):\n                self.gnn_layers.append(GATConv(hidden_dim, hidden_dim // num_heads, \n                                             heads=num_heads, dropout=dropout, concat=True))\n        elif gnn_type == 'transformer':\n            for i in range(num_gnn_layers):\n                self.gnn_layers.append(TransformerConv(hidden_dim, hidden_dim // num_heads,\n                                                     heads=num_heads, dropout=dropout, concat=True))\n        \n        # Layer normalization and dropout\n        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_gnn_layers)])\n        self.dropout = nn.Dropout(dropout)\n        \n        # Final output projection\n        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Initialize parameters\n        self._init_parameters()\n    \n    def _init_parameters(self):\n        \"\"\"Initialize model parameters\"\"\"\n        # Initialize cluster centers with Xavier uniform\n        nn.init.xavier_uniform_(self.cluster_centers)\n        nn.init.xavier_uniform_(self.learnable_r_bar.unsqueeze(0))\n        \n        # Initialize bias parameters\n        nn.init.zeros_(self.grid_bias)\n        nn.init.zeros_(self.region_bias)\n    \n    def compute_correlation_weights(self, \n                                  grid_features: torch.Tensor, \n                                  region_centers: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute correlation weights r_{j,i}^(0) between grid features and region centers\n        Following equation (9) in the paper\n        \n        Args:\n            grid_features: [B, M, hidden_dim] projected grid features v_j^(0)\n            region_centers: [B, K, hidden_dim] region centers r_i^(0)\n        Returns:\n            correlation_weights: [B, M, K] correlation weights r_{j,i}^(0)\n        \"\"\"\n        B, M, D = grid_features.shape\n        K = region_centers.shape[1]\n        \n        # Expand dimensions for broadcasting\n        grid_expanded = grid_features.unsqueeze(2)  # [B, M, 1, D]\n        centers_expanded = region_centers.unsqueeze(1)  # [B, 1, K, D]\n        \n        # Compute dot product v_j^(0) · r_i^(0)\n        dot_products = torch.sum(grid_expanded * centers_expanded, dim=-1)  # [B, M, K]\n        \n        # Add bias terms (b_i and b_j)\n        dot_products = dot_products + self.grid_bias + self.region_bias\n        \n        # Apply softmax to get correlation weights (equation 9)\n        correlation_weights = F.softmax(dot_products, dim=-1)  # [B, M, K]\n        \n        return correlation_weights\n    \n    def aggregate_region_features(self, \n                                grid_features: torch.Tensor,\n                                correlation_weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Aggregate grid features into region features using correlation weights\n        Following equation (10) in the paper\n        \n        Args:\n            grid_features: [B, M, hidden_dim] projected grid features\n            correlation_weights: [B, M, K] correlation weights\n        Returns:\n            aggregated_regions: [B, K, hidden_dim] aggregated region features r_i^(1)\n        \"\"\"\n        B, M, D = grid_features.shape\n        K = correlation_weights.shape[-1]\n        \n        # Weighted sum: Σ(r_{j,i}^(0) * (v_j^(0) - r̄))\n        r_bar_expanded = self.learnable_r_bar.unsqueeze(0).unsqueeze(0).expand(B, M, -1)\n        grid_centered = grid_features - r_bar_expanded  # v_j^(0) - r̄\n        \n        # Compute weighted sum\n        weighted_features = correlation_weights.unsqueeze(-1) * grid_centered.unsqueeze(2)  # [B, M, K, D]\n        aggregated_features = torch.sum(weighted_features, dim=1)  # [B, K, D]\n        \n        # Apply L2 normalization (Norm in equation 10)\n        aggregated_regions = F.normalize(aggregated_features, p=2, dim=-1)\n        \n        return aggregated_regions\n\n    def build_graph(self,\n                   grid_features: torch.Tensor,\n                   region_features: torch.Tensor,\n                   aggregated_regions: torch.Tensor) -> List[Data]:\n        \"\"\"\n        Build graph structure for GNN processing\n        \"\"\"\n        B, M, D = grid_features.shape\n        P = region_features.shape[1]\n        K = aggregated_regions.shape[1]\n        # Lấy device từ một tensor đã có sẵn\n        device = grid_features.device\n        \n        graph_list = []\n        \n        for b in range(B):\n            node_features = torch.cat([\n                grid_features[b],\n                region_features[b],\n                aggregated_regions[b]\n            ], dim=0)\n            \n            edge_indices = []\n            \n            # 1. Grid-to-Grid connections\n            grid_edges = self._build_grid_edges(M, device)\n            edge_indices.append(grid_edges)\n            \n            # 2. Region-to-Region connections\n            region_edges = self._build_region_edges(region_features[b], M, P)\n            edge_indices.append(region_edges)\n            \n            # 3. Grid-to-Aggregated connections\n            grid_agg_edges = self._build_grid_aggregated_edges(\n                grid_features[b], aggregated_regions[b], M, P, K\n            )\n            edge_indices.append(grid_agg_edges)\n            \n            # 4. Region-to-Aggregated connections\n            region_agg_edges = self._build_region_aggregated_edges(\n                region_features[b], aggregated_regions[b], M, P, K\n            )\n            edge_indices.append(region_agg_edges)\n            \n            # Combine all edges\n            all_edge_indices = torch.cat(edge_indices, dim=1)\n            \n            # edge_attr không được sử dụng trong GCNConv, GATConv, TransformerConv mặc định\n            # nên có thể bỏ qua để đơn giản hóa\n            graph_data = Data(\n                x=node_features,\n                edge_index=all_edge_indices\n            )\n            \n            graph_list.append(graph_data)\n        \n        return graph_list\n    \n    # SỬA LỖI: Thêm `device` vào các hàm _build\n    \n    def _build_grid_edges(self, M: int, device: torch.device) -> torch.Tensor:\n        \"\"\"Build spatial connections between grid nodes\"\"\"\n        grid_size = int(math.sqrt(M))\n        edge_indices = []\n        \n        for i in range(grid_size):\n            for j in range(grid_size):\n                current_idx = i * grid_size + j\n                if j < grid_size - 1:\n                    right_idx = i * grid_size + (j + 1)\n                    edge_indices.extend([[current_idx, right_idx], [right_idx, current_idx]])\n                if i < grid_size - 1:\n                    bottom_idx = (i + 1) * grid_size + j\n                    edge_indices.extend([[current_idx, bottom_idx], [bottom_idx, current_idx]])\n        \n        if len(edge_indices) > 0:\n            # SỬA LỖI: Thêm device vào đây\n            return torch.tensor(edge_indices, dtype=torch.long, device=device).t().contiguous()\n        else:\n            return torch.empty((2, 0), dtype=torch.long, device=device)\n\n    def _build_region_edges(self, region_features: torch.Tensor, M: int, P: int) -> torch.Tensor:\n        \"\"\"Build semantic connections between region nodes\"\"\"\n        device = region_features.device # Lấy device\n        \n        region_norm = F.normalize(region_features, p=2, dim=-1)\n        similarity_matrix = torch.mm(region_norm, region_norm.t())\n        \n        k = min(5, P - 1 if P > 1 else 0)\n        if k == 0:\n            return torch.empty((2, 0), dtype=torch.long, device=device)\n            \n        edge_indices = []\n        \n        for i in range(P):\n            similarities = similarity_matrix[i]\n            similarities[i] = -1\n            _, top_indices = torch.topk(similarities, k)\n            \n            for j in top_indices:\n                if similarities[j] > 0.5:\n                    edge_indices.extend([[M + i, M + j], [M + j, M + i]])\n        \n        if len(edge_indices) > 0:\n            # SỬA LỖI: Thêm device vào đây\n            return torch.tensor(edge_indices, dtype=torch.long, device=device).t().contiguous()\n        else:\n            return torch.empty((2, 0), dtype=torch.long, device=device)\n\n    def _build_grid_aggregated_edges(self, \n                                   grid_features: torch.Tensor,\n                                   aggregated_regions: torch.Tensor,\n                                   M: int, P: int, K: int) -> torch.Tensor:\n        \"\"\"Build correlation-based connections between grid and aggregated nodes\"\"\"\n        device = grid_features.device # Lấy device\n        \n        correlation_weights = self.compute_correlation_weights(\n            grid_features.unsqueeze(0), aggregated_regions.unsqueeze(0)\n        ).squeeze(0)\n        \n        edge_indices = []\n        top_k = min(3, K)\n        if top_k == 0:\n            return torch.empty((2, 0), dtype=torch.long, device=device)\n\n        for i in range(M):\n            _, top_indices = torch.topk(correlation_weights[i], top_k)\n            for j in top_indices:\n                if correlation_weights[i, j].item() > 0.1:\n                    edge_indices.extend([[i, M + P + j], [M + P + j, i]])\n        \n        if len(edge_indices) > 0:\n            # SỬA LỖI: Thêm device vào đây\n            return torch.tensor(edge_indices, dtype=torch.long, device=device).t().contiguous()\n        else:\n            return torch.empty((2, 0), dtype=torch.long, device=device)\n\n    def _build_region_aggregated_edges(self,\n                                     region_features: torch.Tensor,\n                                     aggregated_regions: torch.Tensor,\n                                     M: int, P: int, K: int) -> torch.Tensor:\n        \"\"\"Build attention-based connections between region and aggregated nodes\"\"\"\n        device = region_features.device # Lấy device\n\n        region_norm = F.normalize(region_features, p=2, dim=-1)\n        aggregated_norm = F.normalize(aggregated_regions, p=2, dim=-1)\n        attention_scores = torch.mm(region_norm, aggregated_norm.t())\n        \n        edge_indices = []\n        top_k = min(2, K)\n        if top_k == 0:\n             return torch.empty((2, 0), dtype=torch.long, device=device)\n\n        for i in range(P):\n            _, top_indices = torch.topk(attention_scores[i], top_k)\n            for j in top_indices:\n                if attention_scores[i, j].item() > 0.3:\n                    edge_indices.extend([[M + i, M + P + j], [M + P + j, M + i]])\n        \n        if len(edge_indices) > 0:\n            # SỬA LỖI: Thêm device vào đây\n            return torch.tensor(edge_indices, dtype=torch.long, device=device).t().contiguous()\n        else:\n            return torch.empty((2, 0), dtype=torch.long, device=device)\n\n    def forward(self, \n               grid_features: torch.Tensor,\n               region_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        B, M, _ = grid_features.shape\n        P = region_features.shape[1]\n        K = self.num_cluster_centers\n        \n        grid_projected = self.grid_proj(grid_features)\n        region_projected = self.region_proj(region_features)\n        \n        cluster_centers_batch = self.cluster_centers.unsqueeze(0).expand(B, -1, -1)\n        \n        correlation_weights = self.compute_correlation_weights(grid_projected, cluster_centers_batch)\n        aggregated_regions = self.aggregate_region_features(grid_projected, correlation_weights)\n        \n        graph_list = self.build_graph(grid_projected, region_projected, aggregated_regions)\n        graph_batch = Batch.from_data_list(graph_list)\n        \n        x, edge_index = graph_batch.x, graph_batch.edge_index\n        \n        for i, gnn_layer in enumerate(self.gnn_layers):\n            if self.gnn_type == 'gcn':\n                x_new = gnn_layer(x, edge_index)\n            else:\n                x_new = gnn_layer(x, edge_index)\n            \n            x = self.layer_norms[i](x + x_new)\n            x = self.dropout(x)\n        \n        x = self.output_proj(x)\n        \n        node_counts = [M + P + K] * B\n        feature_list = x.split(node_counts)\n        \n        enhanced_grid_features_list = []\n        enhanced_region_features_list = []\n        enhanced_aggregated_regions_list = []\n        \n        for features in feature_list:\n            enhanced_grid_features_list.append(features[:M])\n            enhanced_region_features_list.append(features[M:M+P])\n            enhanced_aggregated_regions_list.append(features[M+P:])\n        \n        enhanced_grid_features = torch.stack(enhanced_grid_features_list)\n        enhanced_region_features = torch.stack(enhanced_region_features_list)\n        enhanced_aggregated_regions = torch.stack(enhanced_aggregated_regions_list)\n        \n        return enhanced_grid_features, enhanced_region_features, enhanced_aggregated_regions\n# Complete integration example\nclass MMGATWithRegionGridAggregator(nn.Module):\n    \"\"\"Complete MMGAT model with Region-Grid Aggregator\"\"\"\n    def __init__(self,\n                 image_size: int = 224,\n                 patch_size: int = 4,\n                 grid_embed_dim: int = 96,\n                 region_feature_dim: int = 512,\n                 hidden_dim: int = 256,\n                 num_regions: int = 50,\n                 num_cluster_centers: int = 10,\n                 gnn_type: str = 'transformer'):\n        super().__init__()\n        \n        # Import the feature extractors (assuming they're available)\n        # from your_module import GridGraphFeature, RegionGraphFeature\n        \n        # Feature extractors\n        # self.grid_extractor = GridGraphFeature(\n        #     image_size=image_size,\n        #     patch_size=patch_size,\n        #     embed_dim=grid_embed_dim,\n        #     gcn_hidden_dim=128\n        # )\n        \n        # self.region_extractor = RegionGraphFeature(\n        #     num_regions=num_regions,\n        #     region_feature_dim=region_feature_dim,\n        #     output_dim=region_feature_dim\n        # )\n        \n        # Region-Grid Aggregator\n        self.region_grid_aggregator = RegionGridAggregator(\n            grid_feature_dim=128,  # Output from GridGraphFeature\n            region_feature_dim=region_feature_dim,\n            hidden_dim=hidden_dim,\n            num_cluster_centers=num_cluster_centers,\n            gnn_type=gnn_type\n        )\n        \n        # Final fusion layer\n        self.final_fusion = nn.Linear(hidden_dim * 3, hidden_dim)  # Grid + Region + Aggregated\n    \n    def forward(self, images: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Complete forward pass\n        Args:\n            images: [B, 3, H, W] input images\n        Returns:\n            fused_features: [B, N_total, hidden_dim] final fused features\n        \"\"\"\n        # Extract features (commented out since modules not imported)\n        # grid_features = self.grid_extractor(images)      # [B, M, 128]\n        # region_features = self.region_extractor(images)  # [B, P, 512]\n        \n        # For demonstration, create dummy features\n        B = images.shape[0]\n        M = (224 // 4) ** 2  # Grid patches\n        P = 50  # Number of regions\n        \n        grid_features = torch.randn(B, M, 128, device=images.device)\n        region_features = torch.randn(B, P, 512, device=images.device)\n        \n        # Apply Region-Grid Aggregator\n        enhanced_grid, enhanced_region, aggregated_region = self.region_grid_aggregator(\n            grid_features, region_features\n        )\n        \n        # Combine all features\n        combined_features = torch.cat([\n            enhanced_grid,      # [B, M, hidden_dim]\n            enhanced_region,    # [B, P, hidden_dim]\n            aggregated_region   # [B, K, hidden_dim]\n        ], dim=1)  # [B, M+P+K, hidden_dim]\n        \n        return combined_features\n\n# Test function\ndef test_region_grid_aggregator():\n    \"\"\"Test the Region-Grid Aggregator implementation\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create model\n    model = RegionGridAggregator(\n        grid_feature_dim=128,\n        region_feature_dim=512,\n        hidden_dim=256,\n        num_cluster_centers=10,\n        gnn_type='transformer',\n        num_gnn_layers=3\n    ).to(device)\n    \n    # Create dummy input features\n    batch_size = 2\n    M = 56 * 56  # Grid features (56x56 grid)\n    P = 50       # Region features\n    \n    grid_features = torch.randn(batch_size, M, 128).to(device)\n    region_features = torch.randn(batch_size, P, 512).to(device)\n    \n    print(f\"Input grid features shape: {grid_features.shape}\")\n    print(f\"Input region features shape: {region_features.shape}\")\n    \n    # Forward pass\n    with torch.no_grad():\n        enhanced_grid, enhanced_region, aggregated_region = model(grid_features, region_features)\n    \n    print(f\"Enhanced grid features shape: {enhanced_grid.shape}\")\n    print(f\"Enhanced region features shape: {enhanced_region.shape}\")\n    print(f\"Aggregated region features shape: {aggregated_region.shape}\")\n    \n    # Test correlation weights computation\n    grid_proj = model.grid_proj(grid_features)\n    cluster_centers_batch = model.cluster_centers.unsqueeze(0).expand(batch_size, -1, -1)\n    correlation_weights = model.compute_correlation_weights(grid_proj, cluster_centers_batch)\n    print(f\"Correlation weights shape: {correlation_weights.shape}\")\n    print(f\"Correlation weights sum (should be ~1.0): {correlation_weights.sum(dim=-1).mean():.3f}\")\n    \n    if device.type == 'cuda':\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n\nif __name__ == \"__main__\":\n    test_region_grid_aggregator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:07:49.974196Z","iopub.execute_input":"2025-06-08T17:07:49.974571Z","iopub.status.idle":"2025-06-08T17:07:51.854370Z","shell.execute_reply.started":"2025-06-08T17:07:49.974544Z","shell.execute_reply":"2025-06-08T17:07:51.853772Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nInput grid features shape: torch.Size([2, 3136, 128])\nInput region features shape: torch.Size([2, 50, 512])\nEnhanced grid features shape: torch.Size([2, 3136, 256])\nEnhanced region features shape: torch.Size([2, 50, 256])\nAggregated region features shape: torch.Size([2, 10, 256])\nCorrelation weights shape: torch.Size([2, 3136, 10])\nCorrelation weights sum (should be ~1.0): 1.000\nGPU memory allocated: 74.36 MB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Grid-semantic aggregator**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}